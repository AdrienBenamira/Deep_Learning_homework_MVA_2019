{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RULES:\n",
    "\n",
    "* Do not create any additional cell\n",
    "\n",
    "* Fill in the blanks\n",
    "\n",
    "* All cells should be runnable (modulo trivial compatibility bugs that we'd fix)\n",
    "\n",
    "* 4 / 20 points will be allocated to the clarity of your code\n",
    "\n",
    "* Efficient code will have a bonus\n",
    "\n",
    "DELIVERABLE:\n",
    "\n",
    "* this notebook\n",
    "* the predictions of the SST test set\n",
    "\n",
    "DO NOT INCLUDE THE DATASETS IN THE DELIVERABLE.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"/media/benamira/19793564030D4273/MCsBackup/3A/MVA/DL/nlp_project/nlp_project/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Monolingual (English) word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2vec():\n",
    "    def __init__(self, fname, nmax=100000):\n",
    "        self.load_wordvec(fname, nmax)\n",
    "        self.word2id = {item:i for i, item in enumerate(self.word2vec.keys())}\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.embeddings = np.array(self.word2vec.values())\n",
    "    \n",
    "    def load_wordvec(self, fname, nmax):\n",
    "        self.word2vec = {}\n",
    "        with io.open(fname, encoding='utf-8') as f:\n",
    "            next(f)\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                self.word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "                if i == (nmax - 1):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(self.word2vec)))\n",
    "\n",
    "    def most_similar(self, w, K=5):\n",
    "        # K most similar words: self.score  -  np.argsort \n",
    "        if type(w)==str:\n",
    "            w = self.word2vec[w]\n",
    "        similarities = [self.score(w, self.word2vec[self.id2word[i]]) for i in range(len(self.word2id))]\n",
    "        closest_k = np.argsort(similarities)[::-1][:K]\n",
    "        results = [self.id2word[i] for i in closest_k]\n",
    "        return results\n",
    "\n",
    "    def score(self, w1, w2):\n",
    "        # cosine similarity: np.dot  -  np.linalg.norm\n",
    "        if type(w1)==str:\n",
    "            e1 = self.word2vec[w1]\n",
    "            e2 = self.word2vec[w2]\n",
    "        else: \n",
    "            e1 = w1\n",
    "            e2 = w2\n",
    "        score = np.dot(e1, e2)  / (np.linalg.norm(e1)*np.linalg.norm(e2))\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 pretrained word vectors\n",
      "cat dog 0.671683666279249\n",
      "dog pet 0.6842064029669219\n",
      "dogs cats 0.7074389328052404\n",
      "paris france 0.7775108541288563\n",
      "germany berlin 0.7420295235998394\n",
      "['cat', 'cats', 'kitty', 'kitten', 'feline']\n",
      "['dog', 'dogs', 'puppy', 'Dog', 'doggie']\n",
      "['dogs', 'dog', 'pooches', 'Dogs', 'doggies']\n",
      "['paris', 'france', 'Paris', 'london', 'berlin']\n",
      "['germany', 'austria', 'europe', 'german', 'berlin']\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=100000)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "for w1, w2 in zip(('cat', 'dog', 'dogs', 'paris', 'germany'), ('dog', 'pet', 'cats', 'france', 'berlin')):\n",
    "    print(w1, w2, w2v.score(w1, w2))\n",
    "for w1 in ['cat', 'dog', 'dogs', 'paris', 'germany']:\n",
    "    print(w2v.most_similar(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoV():\n",
    "    def __init__(self, w2v):\n",
    "        self.w2v = w2v\n",
    "    \n",
    "    def encode(self, sentences, idf=False):\n",
    "        # takes a list of sentences, outputs a numpy array of sentence embeddings\n",
    "        # see TP1 for help\n",
    "        sentemb = []\n",
    "        for sent in sentences:\n",
    "            if idf is False:\n",
    "                # mean of word vectors\n",
    "                sentemb.append(np.mean([self.w2v.word2vec[w] for w in sent if w in self.w2v.word2vec], axis=0))\n",
    "            else:\n",
    "                # idf-weighted mean of word vectors\n",
    "                sentemb.append(np.mean([self.w2v.word2vec[w]*idf[w] for w in sent if w in self.w2v.word2vec], axis=0))\n",
    "        return np.vstack(sentemb)\n",
    "\n",
    "    def most_similar(self, s, sentences, idf=False, K=5):\n",
    "        # get most similar sentences and **print** them\n",
    "        keys = self.encode(sentences, idf)\n",
    "        query = self.encode([s], idf)\n",
    "        \n",
    "        scores = np.dot(keys, query.T)\n",
    "        scores /= np.reshape(np.linalg.norm(keys, axis=1), (-1,1))\n",
    "        closest_sentences = np.argsort(scores[:,0])\n",
    "        closest_sentences = [sentences[i] for i in closest_sentences]\n",
    "        print(closest_sentences[::-1][:K])\n",
    "\n",
    "    def score(self, s1, s2, idf=False):\n",
    "        # cosine similarity: use   np.dot  and  np.linalg.norm\n",
    "        enc1 = self.encode([s1], idf)\n",
    "        enc2 = self.encode([s2], idf)\n",
    "        \n",
    "        res = np.dot(enc1,enc2.T) \\\n",
    "            /(np.linalg.norm(enc1) * np.linalg.norm(enc2)) \n",
    "        \n",
    "        return print(res[0,0])\n",
    "    \n",
    "    def build_idf(self, sentences):\n",
    "        # build the idf dictionary: associate each word to its idf value\n",
    "        idf = {}        \n",
    "        for sent in sentences:\n",
    "            for w in set(sent):\n",
    "                idf[w] = idf.get(w, 0) + 1\n",
    "        \n",
    "        idf = {w:max(1, np.log10(len(sentences) / v)) for (w,v) in idf.items()}\n",
    "        return idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 pretrained word vectors\n",
      "Build idf\n",
      "Done in  0.7116799354553223 s.\n",
      "\n",
      "\n",
      "Start evaluate  BoV-mean\n",
      "[['1', 'smiling', 'african', 'american', 'boy', '.'], ['girl', 'smiling', 'on', 'roller', 'coaster', '.'], ['a', 'boy', 'smiles', 'underwater', '.'], ['two', 'girlfriends', 'smiling', '.'], ['a', 'smiling', 'child', 'swims', '.']]\n",
      "0.5936035444177407\n",
      "Done in  6.17293906211853 s.\n",
      "\n",
      "\n",
      "Start evaluate  BoV-idf\n",
      "[['1', 'smiling', 'african', 'american', 'boy', '.'], ['5', 'women', 'and', '1', 'man', 'are', 'smiling', 'for', 'the', 'camera', '.'], ['2', 'guys', 'facing', 'away', 'from', 'camera', ',', '1', 'girl', 'smiling', 'at', 'camera', 'with', 'blue', 'shirt', ',', '1', 'guy', 'with', 'a', 'beverage', 'with', 'a', 'jacket', 'on', '.'], ['two', 'girlfriends', 'smiling', '.'], ['1', 'man', 'singing', 'and', '1', 'man', 'playing', 'a', 'saxophone', 'in', 'a', 'concert', '.']]\n",
      "0.5004864584233165\n",
      "Done in  10.949394464492798 s.\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=10000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "# Load sentences in \"PATH_TO_DATA/sentences.txt\"\n",
    "file = open(os.path.join(PATH_TO_DATA, 'sentences.txt'), \"r\")\n",
    "sentences = [line.split() for line in file]\n",
    "\n",
    "# Build idf scores for each word\n",
    "print(\"Build idf\")\n",
    "start = time.time()\n",
    "idf = s2v.build_idf(sentences)\n",
    "print(\"Done in \", time.time()- start, \"s.\")\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "print(\"Start evaluate  BoV-mean\")\n",
    "start = time.time()\n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences)  # BoV-mean\n",
    "s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13])\n",
    "print(\"Done in \", time.time()- start, \"s.\")\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"Start evaluate  BoV-idf\")\n",
    "start = time.time()\n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences, idf)  # BoV-idf\n",
    "s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13], idf)\n",
    "print(\"Done in \", time.time()- start, \"s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Multilingual (English-French) word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a bilingual dictionary of size V_a (e.g French-English).\n",
    "\n",
    "Let's define **X** and **Y** the **French** and **English** matrices.\n",
    "\n",
    "They contain the embeddings associated to the words in the bilingual dictionary.\n",
    "\n",
    "We want to find a **mapping W** that will project the source word space (e.g French) to the target word space (e.g English).\n",
    "\n",
    "Procrustes : **W\\* = argmin || W.X - Y ||  s.t  W^T.W = Id**\n",
    "has a closed form solution:\n",
    "**W = U.V^T  where  U.Sig.V^T = SVD(Y.X^T)**\n",
    "\n",
    "In what follows, you are asked to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n",
      "Loaded 50000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 1 - Download and load 50k first vectors of\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec\n",
    "\n",
    "# TYPE CODE HERE\n",
    "w2v_en = Word2vec(os.path.join(PATH_TO_DATA, 'wiki.en.vec'), nmax=50000)\n",
    "w2v_fr = Word2vec(os.path.join(PATH_TO_DATA, 'wiki.fr.vec'), nmax=50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 18970)\n",
      "(300, 18970)\n"
     ]
    }
   ],
   "source": [
    "# 2 - Get words that appear in both vocabs (= identical character strings)\n",
    "#     Use it to create the matrix X and Y (of aligned embeddings for these words)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "dictX = {w: w2v_fr.word2vec[w] for w in w2v_en.word2vec if w in w2v_fr.word2vec}\n",
    "dictY = {w: w2v_en.word2vec[w] for w in dictX}\n",
    "\n",
    "X = np.array([dictX[w] for w in dictX]).transpose()\n",
    "Y = np.array([dictY[w] for w in dictX]).transpose()\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Solve the Procrustes using the scipy package and: scipy.linalg.svd() and get the optimal W\n",
    "#     Now W*French_vector is in the same space as English_vector\n",
    "\n",
    "# TYPE CODE HERE\n",
    "U, S, V = np.linalg.svd(np.dot(Y, X.T), full_matrices=False)\n",
    "W_fr2en = np.dot(U, V)\n",
    "W_en2fr = np.linalg.inv(W_fr2en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French -> English, most similar : \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>results</th>\n",
       "      <th>time</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[cat, rabbit, hamster, feline, poodle]</td>\n",
       "      <td>0.605424</td>\n",
       "      <td>chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[école, ecole, school, supérieure, polytechnique]</td>\n",
       "      <td>0.607101</td>\n",
       "      <td>école</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[eat, meal, eating, eaten, ate]</td>\n",
       "      <td>0.611501</td>\n",
       "      <td>manger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[during, periods, prolonged, months, intermittently]</td>\n",
       "      <td>0.594387</td>\n",
       "      <td>pendant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[proud, fond, arrogant, amiable, brave]</td>\n",
       "      <td>0.597199</td>\n",
       "      <td>fier</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                results      time    words\n",
       "0                [cat, rabbit, hamster, feline, poodle]  0.605424     chat\n",
       "1     [école, ecole, school, supérieure, polytechnique]  0.607101    école\n",
       "2                       [eat, meal, eating, eaten, ate]  0.611501   manger\n",
       "3  [during, periods, prolonged, months, intermittently]  0.594387  pendant\n",
       "4               [proud, fond, arrogant, amiable, brave]  0.597199     fier"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English -> French, most similar :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>results</th>\n",
       "      <th>time</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[cat, chat, dog, chats, chien]</td>\n",
       "      <td>0.614306</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[voiture, voitures, automobile, porsche, automobiles]</td>\n",
       "      <td>0.594589</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[nom, dénomination, noms, diminutif, surnom]</td>\n",
       "      <td>0.614192</td>\n",
       "      <td>name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[roi, king, rois, reine, trône]</td>\n",
       "      <td>0.609755</td>\n",
       "      <td>king</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[beautiful, beauty, beauté, charmante, beautés]</td>\n",
       "      <td>0.593305</td>\n",
       "      <td>beautiful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 results      time      words\n",
       "0                         [cat, chat, dog, chats, chien]  0.614306        cat\n",
       "1  [voiture, voitures, automobile, porsche, automobiles]  0.594589        car\n",
       "2           [nom, dénomination, noms, diminutif, surnom]  0.614192       name\n",
       "3                        [roi, king, rois, reine, trône]  0.609755       king\n",
       "4        [beautiful, beauty, beauté, charmante, beautés]  0.593305  beautiful"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4 - After alignment with W, give examples of English nearest neighbors of some French words (and vice versa)\n",
    "#     You will be evaluated on that part and the code above\n",
    "# TYPE CODE HERE\n",
    "res_fr2en = {\"words\" : ['chat', 'école', 'manger', 'pendant','fier'], \"results\":[], \"time\":[] }\n",
    "for w in res_fr2en[\"words\"]:\n",
    "    start = time.time()\n",
    "    res_fr2en[\"results\"].append(w2v_en.most_similar(np.dot(W_fr2en, w2v_fr.word2vec[w])))\n",
    "    res_fr2en[\"time\"].append(time.time()-start)\n",
    "    \n",
    "# 4 - After alignment with W, give examples of English nearest neighbors of some French words (and vice versa)\n",
    "#     You will be evaluated on that part and the code above\n",
    "\n",
    "\n",
    "\n",
    "# TYPE CODE HERE\n",
    "res_en2fr = {\"words\" : ['cat', 'car', 'name', 'king','beautiful'], \"results\":[], \"time\":[] }\n",
    "for w in res_en2fr[\"words\"]:\n",
    "    start = time.time()\n",
    "    res_en2fr[\"results\"].append(w2v_fr.most_similar(np.dot(W_en2fr, w2v_en.word2vec[w])))\n",
    "    res_en2fr[\"time\"].append(time.time()-start)\n",
    "    \n",
    "\n",
    "pd.set_option('max_colwidth', 800)\n",
    "print('French -> English, most similar : ')\n",
    "df_fr2en = pd.DataFrame(res_fr2en)\n",
    "display(df_fr2en)\n",
    "\n",
    "print('English -> French, most similar :')\n",
    "df_en2fr = pd.DataFrame(res_en2fr)\n",
    "display(df_en2fr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to dive deeper on this subject: https://github.com/facebookresearch/MUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sentence classification with BoV and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sentences...\n",
      "Loaded 8544 sentences in  0.08902096748352051 s.\n",
      "Loading sentences...\n",
      "Loaded 1101 sentences in  0.006530046463012695 s.\n",
      "Loading sentences...\n",
      "Loaded 2210 sentences in  0.006242513656616211 s.\n"
     ]
    }
   ],
   "source": [
    "# 1 - Ô\n",
    "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "def import_sentences(path, label_bol=False):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    start = time.time()\n",
    "    print('Loading sentences...')\n",
    "    file = open(os.path.join(path), \"r\")\n",
    "    for line in file:\n",
    "        sentences.append(line.split()[1:]) \n",
    "        if label_bol:\n",
    "            labels.append(line.split()[0])\n",
    "    labels = [int(labels[i]) for i in range(len(labels))]\n",
    "    print('Loaded', len(sentences), 'sentences in ', str(time.time()-start), \"s.\")\n",
    "    if label_bol:\n",
    "        return sentences, labels\n",
    "    return sentences\n",
    "\n",
    "\n",
    "train_sst, train_labels = import_sentences(PATH_TO_DATA + \"/SST/stsa.fine.train\", label_bol=True)\n",
    "val_sst, val_labels = import_sentences(PATH_TO_DATA + \"/SST/stsa.fine.dev\", label_bol=True)\n",
    "test_sst = import_sentences(PATH_TO_DATA + \"/SST/stsa.fine.test.X\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Encode sentences with the BoV model above\n",
    "\n",
    "# TYPE CODE HERE\n",
    "train_embeddings = s2v.encode(train_sst)\n",
    "val_embeddings = s2v.encode(val_sst)\n",
    "test_embeddings = s2v.encode(test_sst)\n",
    "\n",
    "idf = s2v.build_idf(train_sst + val_sst + test_sst)\n",
    "\n",
    "train_embeddings_idf = s2v.encode(train_sst, idf)\n",
    "val_embeddings_idf = s2v.encode(val_sst, idf)\n",
    "test_embeddings_idf = s2v.encode(test_sst, idf)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(train_embeddings)\n",
    "train_embeddings_norm = scaler.transform(train_embeddings)\n",
    "val_embeddings_norm = scaler.transform(val_embeddings)\n",
    "test_embeddings_norm = scaler.transform(test_embeddings)\n",
    "\n",
    "scaler_idf = preprocessing.StandardScaler().fit(train_embeddings_idf)\n",
    "train_embeddings_norm_idf = scaler_idf.transform(train_embeddings_idf)\n",
    "val_embeddings_norm_idf = scaler_idf.transform(val_embeddings_idf)\n",
    "test_embeddings_norm_idf = scaler_idf.transform(test_embeddings_idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start trainning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>weigth</th>\n",
       "      <th>word norm</th>\n",
       "      <th>weigth norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>0.418422</td>\n",
       "      <td>0.444522</td>\n",
       "      <td>0.469101</td>\n",
       "      <td>0.457865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.455875</td>\n",
       "      <td>0.456344</td>\n",
       "      <td>0.470623</td>\n",
       "      <td>0.458099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>0.462430</td>\n",
       "      <td>0.457514</td>\n",
       "      <td>0.470272</td>\n",
       "      <td>0.457982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20.0</th>\n",
       "      <td>0.461728</td>\n",
       "      <td>0.459036</td>\n",
       "      <td>0.470272</td>\n",
       "      <td>0.457982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.0</th>\n",
       "      <td>0.464419</td>\n",
       "      <td>0.457982</td>\n",
       "      <td>0.470154</td>\n",
       "      <td>0.457982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word   weigth   word norm  weigth norm\n",
       "0.1    0.418422  0.444522   0.469101     0.457865\n",
       "1.0    0.455875  0.456344   0.470623     0.458099\n",
       "10.0   0.462430  0.457514   0.470272     0.457982\n",
       "20.0   0.461728  0.459036   0.470272     0.457982\n",
       "100.0  0.464419  0.457982   0.470154     0.457982"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>weigth</th>\n",
       "      <th>word norm</th>\n",
       "      <th>weigth norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>0.310285</td>\n",
       "      <td>0.381331</td>\n",
       "      <td>0.443472</td>\n",
       "      <td>0.428945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.406111</td>\n",
       "      <td>0.422671</td>\n",
       "      <td>0.446250</td>\n",
       "      <td>0.430234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>0.432636</td>\n",
       "      <td>0.427916</td>\n",
       "      <td>0.446278</td>\n",
       "      <td>0.430002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20.0</th>\n",
       "      <td>0.434147</td>\n",
       "      <td>0.430258</td>\n",
       "      <td>0.446332</td>\n",
       "      <td>0.430002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.0</th>\n",
       "      <td>0.435687</td>\n",
       "      <td>0.429684</td>\n",
       "      <td>0.446182</td>\n",
       "      <td>0.430002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word   weigth   word norm  weigth norm\n",
       "0.1    0.310285  0.381331   0.443472     0.428945\n",
       "1.0    0.406111  0.422671   0.446250     0.430234\n",
       "10.0   0.432636  0.427916   0.446278     0.430002\n",
       "20.0   0.434147  0.430258   0.446332     0.430002\n",
       "100.0  0.435687  0.429684   0.446182     0.430002"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on train\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>weigth</th>\n",
       "      <th>word norm</th>\n",
       "      <th>weigth norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>0.390554</td>\n",
       "      <td>0.390554</td>\n",
       "      <td>0.405086</td>\n",
       "      <td>0.395095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.397820</td>\n",
       "      <td>0.397820</td>\n",
       "      <td>0.403270</td>\n",
       "      <td>0.391462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>0.405995</td>\n",
       "      <td>0.393279</td>\n",
       "      <td>0.403270</td>\n",
       "      <td>0.391462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20.0</th>\n",
       "      <td>0.401453</td>\n",
       "      <td>0.391462</td>\n",
       "      <td>0.404178</td>\n",
       "      <td>0.391462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.0</th>\n",
       "      <td>0.404178</td>\n",
       "      <td>0.389646</td>\n",
       "      <td>0.403270</td>\n",
       "      <td>0.391462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word   weigth   word norm  weigth norm\n",
       "0.1    0.390554  0.390554   0.405086     0.395095\n",
       "1.0    0.397820  0.397820   0.403270     0.391462\n",
       "10.0   0.405995  0.393279   0.403270     0.391462\n",
       "20.0   0.401453  0.391462   0.404178     0.391462\n",
       "100.0  0.404178  0.389646   0.403270     0.391462"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>weigth</th>\n",
       "      <th>word norm</th>\n",
       "      <th>weigth norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>0.284929</td>\n",
       "      <td>0.328306</td>\n",
       "      <td>0.377329</td>\n",
       "      <td>0.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.346753</td>\n",
       "      <td>0.367808</td>\n",
       "      <td>0.375392</td>\n",
       "      <td>0.364754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>0.370866</td>\n",
       "      <td>0.364964</td>\n",
       "      <td>0.375385</td>\n",
       "      <td>0.364605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20.0</th>\n",
       "      <td>0.370122</td>\n",
       "      <td>0.363950</td>\n",
       "      <td>0.376095</td>\n",
       "      <td>0.364605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.0</th>\n",
       "      <td>0.372417</td>\n",
       "      <td>0.360801</td>\n",
       "      <td>0.375495</td>\n",
       "      <td>0.364605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word   weigth   word norm  weigth norm\n",
       "0.1    0.284929  0.328306   0.377329     0.370000\n",
       "1.0    0.346753  0.367808   0.375392     0.364754\n",
       "10.0   0.370866  0.364964   0.375385     0.364605\n",
       "20.0   0.370122  0.363950   0.376095     0.364605\n",
       "100.0  0.372417  0.360801   0.375495     0.364605"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on validation\n",
      "done in  38.226515769958496 s.\n",
      "l2 optimal: 0.1\n",
      "config optimal optimal: 2\n",
      "accuracy et f1-score val pour model optimal 0.405086285195277 0.37732850022868175\n"
     ]
    }
   ],
   "source": [
    "# 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
    "#     (consider tuning the L2 regularization on the dev set)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "def metrics(classifier, embeddings, labels):\n",
    "    labels_pred = classifier.predict(embeddings)\n",
    "    acc = accuracy_score(labels, labels_pred)\n",
    "    f1 = f1_score(labels, labels_pred, average='macro')\n",
    "    return acc, f1\n",
    "def plot_res(all_mes, header, name):\n",
    "    df = pd.DataFrame(dict(list(enumerate(all_mes))))\n",
    "    df.columns = l2_values\n",
    "    df_fin = df.T\n",
    "    df_fin.columns = header\n",
    "    display(df_fin)\n",
    "    print(name)\n",
    "\n",
    "header = np.array([\"word\",\"weigth \",\"word norm\",\"weigth norm\"])\n",
    "acc_max = 0\n",
    "f1_max =0\n",
    "l2_values = [0.1, 1, 10, 20, 100]\n",
    "all_acc = []\n",
    "all_f1 = []\n",
    "all_acc_train = []\n",
    "all_f1_train = []\n",
    "print(\"start trainning\")\n",
    "start = time.time()\n",
    "for l2_c in l2_values:\n",
    "    all_acc_col = []\n",
    "    all_f1_col = []\n",
    "    all_acc_col_train = []\n",
    "    all_f1_col_train = []\n",
    "    for idx, (train_data,val_data) in enumerate([(train_embeddings, val_embeddings),\n",
    "                       (train_embeddings_idf, val_embeddings_idf),\n",
    "                       (train_embeddings_norm, val_embeddings_norm),\n",
    "                       (train_embeddings_norm_idf,val_embeddings_norm_idf)]):\n",
    "        clf = LogisticRegression(random_state=0, solver='lbfgs', C=l2_c,penalty='l2',tol=1e-6,\n",
    "                                 multi_class='multinomial').fit(train_data, train_labels)\n",
    "        acc_train, f1_train = metrics(clf, train_data, train_labels)\n",
    "        acc, f1 = metrics(clf, val_data, val_labels)\n",
    "        all_acc_col.append(acc)\n",
    "        all_f1_col.append(f1)\n",
    "        all_acc_col_train.append(acc_train)\n",
    "        all_f1_col_train.append(f1_train)\n",
    "        if acc>=acc_max:\n",
    "            if f1>=f1_max:\n",
    "                best_model = clf\n",
    "                best_l2_c = l2_c\n",
    "                best_idx = idx\n",
    "                acc_max=acc\n",
    "                f1_max = f1\n",
    "    all_acc_train.append(all_acc_col_train)\n",
    "    all_f1_train.append(all_f1_col_train)\n",
    "    all_acc.append(all_acc_col)\n",
    "    all_f1.append(all_f1_col)\n",
    "\n",
    "\n",
    "plot_res(all_acc_train, header, \"Accuracy on train\")\n",
    "plot_res(all_f1_train, header, \"F1-score on train\")\n",
    "plot_res(all_acc, header, \"Accuracy on validation\")\n",
    "plot_res(all_f1, header, \"F1-score on validation\")\n",
    "\n",
    "\n",
    "print(\"done in \",time.time() - start ,\"s.\")\n",
    "        \n",
    "print('l2 optimal:', best_l2_c)\n",
    "print('config optimal optimal:', best_idx)\n",
    "print('accuracy et f1-score val pour model optimal', acc_max, f1_max)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred  2210  cases\n"
     ]
    }
   ],
   "source": [
    "# 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
    "#     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
    "#     You will be evaluated on the results of the test set.\n",
    "\n",
    "# TYPE CODE HERE\n",
    "# Save the test predictions\n",
    "labels_test_pred = best_model.predict(test_embeddings_norm)\n",
    "print(\"pred \", len(labels_test_pred) ,\" cases\")\n",
    "file = \"./logreg_bov_y_test_sst.txt\"\n",
    "with open(file, 'w') as f:\n",
    "        for label in labels_test_pred:\n",
    "            f.write(str(label) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8544, 300)\n",
      "[1]\tvalid_0's multi_logloss: 1.57174\n",
      "[2]\tvalid_0's multi_logloss: 1.57073\n",
      "[3]\tvalid_0's multi_logloss: 1.56974\n",
      "[4]\tvalid_0's multi_logloss: 1.56877\n",
      "[5]\tvalid_0's multi_logloss: 1.56782\n",
      "[6]\tvalid_0's multi_logloss: 1.56689\n",
      "[7]\tvalid_0's multi_logloss: 1.56602\n",
      "[8]\tvalid_0's multi_logloss: 1.56513\n",
      "[9]\tvalid_0's multi_logloss: 1.56426\n",
      "[10]\tvalid_0's multi_logloss: 1.56343\n",
      "[11]\tvalid_0's multi_logloss: 1.56258\n",
      "[12]\tvalid_0's multi_logloss: 1.56178\n",
      "[13]\tvalid_0's multi_logloss: 1.56089\n",
      "[14]\tvalid_0's multi_logloss: 1.56011\n",
      "[15]\tvalid_0's multi_logloss: 1.55924\n",
      "[16]\tvalid_0's multi_logloss: 1.55846\n",
      "[17]\tvalid_0's multi_logloss: 1.55763\n",
      "[18]\tvalid_0's multi_logloss: 1.5568\n",
      "[19]\tvalid_0's multi_logloss: 1.55603\n",
      "[20]\tvalid_0's multi_logloss: 1.55524\n",
      "[21]\tvalid_0's multi_logloss: 1.55459\n",
      "[22]\tvalid_0's multi_logloss: 1.55379\n",
      "[23]\tvalid_0's multi_logloss: 1.55303\n",
      "[24]\tvalid_0's multi_logloss: 1.55242\n",
      "[25]\tvalid_0's multi_logloss: 1.55171\n",
      "[26]\tvalid_0's multi_logloss: 1.55102\n",
      "[27]\tvalid_0's multi_logloss: 1.55039\n",
      "[28]\tvalid_0's multi_logloss: 1.54964\n",
      "[29]\tvalid_0's multi_logloss: 1.54887\n",
      "[30]\tvalid_0's multi_logloss: 1.54825\n",
      "[31]\tvalid_0's multi_logloss: 1.54748\n",
      "[32]\tvalid_0's multi_logloss: 1.54677\n",
      "[33]\tvalid_0's multi_logloss: 1.54607\n",
      "[34]\tvalid_0's multi_logloss: 1.54548\n",
      "[35]\tvalid_0's multi_logloss: 1.54476\n",
      "[36]\tvalid_0's multi_logloss: 1.54409\n",
      "[37]\tvalid_0's multi_logloss: 1.54347\n",
      "[38]\tvalid_0's multi_logloss: 1.54281\n",
      "[39]\tvalid_0's multi_logloss: 1.54219\n",
      "[40]\tvalid_0's multi_logloss: 1.54164\n",
      "[41]\tvalid_0's multi_logloss: 1.54099\n",
      "[42]\tvalid_0's multi_logloss: 1.54048\n",
      "[43]\tvalid_0's multi_logloss: 1.53991\n",
      "[44]\tvalid_0's multi_logloss: 1.53929\n",
      "[45]\tvalid_0's multi_logloss: 1.53875\n",
      "[46]\tvalid_0's multi_logloss: 1.53824\n",
      "[47]\tvalid_0's multi_logloss: 1.5377\n",
      "[48]\tvalid_0's multi_logloss: 1.53714\n",
      "[49]\tvalid_0's multi_logloss: 1.5367\n",
      "[50]\tvalid_0's multi_logloss: 1.53608\n",
      "[51]\tvalid_0's multi_logloss: 1.53553\n",
      "[52]\tvalid_0's multi_logloss: 1.53507\n",
      "[53]\tvalid_0's multi_logloss: 1.53455\n",
      "[54]\tvalid_0's multi_logloss: 1.53405\n",
      "[55]\tvalid_0's multi_logloss: 1.53357\n",
      "[56]\tvalid_0's multi_logloss: 1.53301\n",
      "[57]\tvalid_0's multi_logloss: 1.53253\n",
      "[58]\tvalid_0's multi_logloss: 1.53197\n",
      "[59]\tvalid_0's multi_logloss: 1.53151\n",
      "[60]\tvalid_0's multi_logloss: 1.53097\n",
      "[61]\tvalid_0's multi_logloss: 1.53042\n",
      "[62]\tvalid_0's multi_logloss: 1.52999\n",
      "[63]\tvalid_0's multi_logloss: 1.52947\n",
      "[64]\tvalid_0's multi_logloss: 1.52897\n",
      "[65]\tvalid_0's multi_logloss: 1.52839\n",
      "[66]\tvalid_0's multi_logloss: 1.52784\n",
      "[67]\tvalid_0's multi_logloss: 1.52742\n",
      "[68]\tvalid_0's multi_logloss: 1.52689\n",
      "[69]\tvalid_0's multi_logloss: 1.52642\n",
      "[70]\tvalid_0's multi_logloss: 1.52599\n",
      "[71]\tvalid_0's multi_logloss: 1.52553\n",
      "[72]\tvalid_0's multi_logloss: 1.52499\n",
      "[73]\tvalid_0's multi_logloss: 1.52452\n",
      "[74]\tvalid_0's multi_logloss: 1.52418\n",
      "[75]\tvalid_0's multi_logloss: 1.52372\n",
      "[76]\tvalid_0's multi_logloss: 1.52329\n",
      "[77]\tvalid_0's multi_logloss: 1.52288\n",
      "[78]\tvalid_0's multi_logloss: 1.52237\n",
      "[79]\tvalid_0's multi_logloss: 1.52192\n",
      "[80]\tvalid_0's multi_logloss: 1.52146\n",
      "[81]\tvalid_0's multi_logloss: 1.52108\n",
      "[82]\tvalid_0's multi_logloss: 1.52061\n",
      "[83]\tvalid_0's multi_logloss: 1.52019\n",
      "[84]\tvalid_0's multi_logloss: 1.51973\n",
      "[85]\tvalid_0's multi_logloss: 1.5193\n",
      "[86]\tvalid_0's multi_logloss: 1.51892\n",
      "[87]\tvalid_0's multi_logloss: 1.51848\n",
      "[88]\tvalid_0's multi_logloss: 1.51815\n",
      "[89]\tvalid_0's multi_logloss: 1.51775\n",
      "[90]\tvalid_0's multi_logloss: 1.51745\n",
      "[91]\tvalid_0's multi_logloss: 1.51712\n",
      "[92]\tvalid_0's multi_logloss: 1.51673\n",
      "[93]\tvalid_0's multi_logloss: 1.51636\n",
      "[94]\tvalid_0's multi_logloss: 1.51591\n",
      "[95]\tvalid_0's multi_logloss: 1.51554\n",
      "[96]\tvalid_0's multi_logloss: 1.51519\n",
      "[97]\tvalid_0's multi_logloss: 1.51481\n",
      "[98]\tvalid_0's multi_logloss: 1.51444\n",
      "[99]\tvalid_0's multi_logloss: 1.51405\n",
      "[100]\tvalid_0's multi_logloss: 1.51368\n",
      "[101]\tvalid_0's multi_logloss: 1.51331\n",
      "[102]\tvalid_0's multi_logloss: 1.51289\n",
      "[103]\tvalid_0's multi_logloss: 1.51255\n",
      "[104]\tvalid_0's multi_logloss: 1.51217\n",
      "[105]\tvalid_0's multi_logloss: 1.51187\n",
      "[106]\tvalid_0's multi_logloss: 1.51151\n",
      "[107]\tvalid_0's multi_logloss: 1.51122\n",
      "[108]\tvalid_0's multi_logloss: 1.51086\n",
      "[109]\tvalid_0's multi_logloss: 1.51048\n",
      "[110]\tvalid_0's multi_logloss: 1.51022\n",
      "[111]\tvalid_0's multi_logloss: 1.50988\n",
      "[112]\tvalid_0's multi_logloss: 1.50948\n",
      "[113]\tvalid_0's multi_logloss: 1.50916\n",
      "[114]\tvalid_0's multi_logloss: 1.50886\n",
      "[115]\tvalid_0's multi_logloss: 1.5086\n",
      "[116]\tvalid_0's multi_logloss: 1.50826\n",
      "[117]\tvalid_0's multi_logloss: 1.50788\n",
      "[118]\tvalid_0's multi_logloss: 1.50759\n",
      "[119]\tvalid_0's multi_logloss: 1.50733\n",
      "[120]\tvalid_0's multi_logloss: 1.50698\n",
      "[121]\tvalid_0's multi_logloss: 1.50667\n",
      "[122]\tvalid_0's multi_logloss: 1.50634\n",
      "[123]\tvalid_0's multi_logloss: 1.5061\n",
      "[124]\tvalid_0's multi_logloss: 1.50574\n",
      "[125]\tvalid_0's multi_logloss: 1.50546\n",
      "[126]\tvalid_0's multi_logloss: 1.50513\n",
      "[127]\tvalid_0's multi_logloss: 1.50478\n",
      "[128]\tvalid_0's multi_logloss: 1.50455\n",
      "[129]\tvalid_0's multi_logloss: 1.50431\n",
      "[130]\tvalid_0's multi_logloss: 1.50401\n",
      "[131]\tvalid_0's multi_logloss: 1.50369\n",
      "[132]\tvalid_0's multi_logloss: 1.50338\n",
      "[133]\tvalid_0's multi_logloss: 1.50308\n",
      "[134]\tvalid_0's multi_logloss: 1.50277\n",
      "[135]\tvalid_0's multi_logloss: 1.50242\n",
      "[136]\tvalid_0's multi_logloss: 1.50211\n",
      "[137]\tvalid_0's multi_logloss: 1.50183\n",
      "[138]\tvalid_0's multi_logloss: 1.50154\n",
      "[139]\tvalid_0's multi_logloss: 1.50128\n",
      "[140]\tvalid_0's multi_logloss: 1.50093\n",
      "[141]\tvalid_0's multi_logloss: 1.50061\n",
      "[142]\tvalid_0's multi_logloss: 1.50034\n",
      "[143]\tvalid_0's multi_logloss: 1.50005\n",
      "[144]\tvalid_0's multi_logloss: 1.49976\n",
      "[145]\tvalid_0's multi_logloss: 1.49938\n",
      "[146]\tvalid_0's multi_logloss: 1.49905\n",
      "[147]\tvalid_0's multi_logloss: 1.49884\n",
      "[148]\tvalid_0's multi_logloss: 1.49856\n",
      "[149]\tvalid_0's multi_logloss: 1.49824\n",
      "[150]\tvalid_0's multi_logloss: 1.4979\n",
      "[151]\tvalid_0's multi_logloss: 1.49761\n",
      "[152]\tvalid_0's multi_logloss: 1.49732\n",
      "[153]\tvalid_0's multi_logloss: 1.49706\n",
      "[154]\tvalid_0's multi_logloss: 1.49678\n",
      "[155]\tvalid_0's multi_logloss: 1.49659\n",
      "[156]\tvalid_0's multi_logloss: 1.49625\n",
      "[157]\tvalid_0's multi_logloss: 1.49591\n",
      "[158]\tvalid_0's multi_logloss: 1.49571\n",
      "[159]\tvalid_0's multi_logloss: 1.4954\n",
      "[160]\tvalid_0's multi_logloss: 1.49508\n",
      "[161]\tvalid_0's multi_logloss: 1.4948\n",
      "[162]\tvalid_0's multi_logloss: 1.49454\n",
      "[163]\tvalid_0's multi_logloss: 1.49434\n",
      "[164]\tvalid_0's multi_logloss: 1.494\n",
      "[165]\tvalid_0's multi_logloss: 1.4938\n",
      "[166]\tvalid_0's multi_logloss: 1.49348\n",
      "[167]\tvalid_0's multi_logloss: 1.49314\n",
      "[168]\tvalid_0's multi_logloss: 1.49289\n",
      "[169]\tvalid_0's multi_logloss: 1.49264\n",
      "[170]\tvalid_0's multi_logloss: 1.49236\n",
      "[171]\tvalid_0's multi_logloss: 1.49211\n",
      "[172]\tvalid_0's multi_logloss: 1.49185\n",
      "[173]\tvalid_0's multi_logloss: 1.49162\n",
      "[174]\tvalid_0's multi_logloss: 1.49128\n",
      "[175]\tvalid_0's multi_logloss: 1.49108\n",
      "[176]\tvalid_0's multi_logloss: 1.49082\n",
      "[177]\tvalid_0's multi_logloss: 1.4905\n",
      "[178]\tvalid_0's multi_logloss: 1.49015\n",
      "[179]\tvalid_0's multi_logloss: 1.48991\n",
      "[180]\tvalid_0's multi_logloss: 1.48973\n",
      "[181]\tvalid_0's multi_logloss: 1.48952\n",
      "[182]\tvalid_0's multi_logloss: 1.48924\n",
      "[183]\tvalid_0's multi_logloss: 1.48897\n",
      "[184]\tvalid_0's multi_logloss: 1.48868\n",
      "[185]\tvalid_0's multi_logloss: 1.48844\n",
      "[186]\tvalid_0's multi_logloss: 1.48825\n",
      "[187]\tvalid_0's multi_logloss: 1.48793\n",
      "[188]\tvalid_0's multi_logloss: 1.48768\n",
      "[189]\tvalid_0's multi_logloss: 1.48733\n",
      "[190]\tvalid_0's multi_logloss: 1.48707\n",
      "[191]\tvalid_0's multi_logloss: 1.48679\n",
      "[192]\tvalid_0's multi_logloss: 1.48654\n",
      "[193]\tvalid_0's multi_logloss: 1.48637\n",
      "[194]\tvalid_0's multi_logloss: 1.48614\n",
      "[195]\tvalid_0's multi_logloss: 1.48595\n",
      "[196]\tvalid_0's multi_logloss: 1.48568\n",
      "[197]\tvalid_0's multi_logloss: 1.48541\n",
      "[198]\tvalid_0's multi_logloss: 1.48514\n",
      "[199]\tvalid_0's multi_logloss: 1.48484\n",
      "[200]\tvalid_0's multi_logloss: 1.48456\n",
      "[201]\tvalid_0's multi_logloss: 1.48429\n",
      "[202]\tvalid_0's multi_logloss: 1.48418\n",
      "[203]\tvalid_0's multi_logloss: 1.48398\n",
      "[204]\tvalid_0's multi_logloss: 1.48366\n",
      "[205]\tvalid_0's multi_logloss: 1.48338\n",
      "[206]\tvalid_0's multi_logloss: 1.48315\n",
      "[207]\tvalid_0's multi_logloss: 1.48305\n",
      "[208]\tvalid_0's multi_logloss: 1.48274\n",
      "[209]\tvalid_0's multi_logloss: 1.48253\n",
      "[210]\tvalid_0's multi_logloss: 1.48228\n",
      "[211]\tvalid_0's multi_logloss: 1.48206\n",
      "[212]\tvalid_0's multi_logloss: 1.48178\n",
      "[213]\tvalid_0's multi_logloss: 1.48154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[214]\tvalid_0's multi_logloss: 1.48133\n",
      "[215]\tvalid_0's multi_logloss: 1.48108\n",
      "[216]\tvalid_0's multi_logloss: 1.48083\n",
      "[217]\tvalid_0's multi_logloss: 1.48061\n",
      "[218]\tvalid_0's multi_logloss: 1.4804\n",
      "[219]\tvalid_0's multi_logloss: 1.48023\n",
      "[220]\tvalid_0's multi_logloss: 1.48008\n",
      "[221]\tvalid_0's multi_logloss: 1.47984\n",
      "[222]\tvalid_0's multi_logloss: 1.47958\n",
      "[223]\tvalid_0's multi_logloss: 1.47925\n",
      "[224]\tvalid_0's multi_logloss: 1.47903\n",
      "[225]\tvalid_0's multi_logloss: 1.47886\n",
      "[226]\tvalid_0's multi_logloss: 1.4786\n",
      "[227]\tvalid_0's multi_logloss: 1.47845\n",
      "[228]\tvalid_0's multi_logloss: 1.47817\n",
      "[229]\tvalid_0's multi_logloss: 1.478\n",
      "[230]\tvalid_0's multi_logloss: 1.4778\n",
      "[231]\tvalid_0's multi_logloss: 1.47763\n",
      "[232]\tvalid_0's multi_logloss: 1.47737\n",
      "[233]\tvalid_0's multi_logloss: 1.47715\n",
      "[234]\tvalid_0's multi_logloss: 1.47695\n",
      "[235]\tvalid_0's multi_logloss: 1.47675\n",
      "[236]\tvalid_0's multi_logloss: 1.47654\n",
      "[237]\tvalid_0's multi_logloss: 1.47634\n",
      "[238]\tvalid_0's multi_logloss: 1.47618\n",
      "[239]\tvalid_0's multi_logloss: 1.47597\n",
      "[240]\tvalid_0's multi_logloss: 1.4758\n",
      "[241]\tvalid_0's multi_logloss: 1.47564\n",
      "[242]\tvalid_0's multi_logloss: 1.47543\n",
      "[243]\tvalid_0's multi_logloss: 1.47519\n",
      "[244]\tvalid_0's multi_logloss: 1.47499\n",
      "[245]\tvalid_0's multi_logloss: 1.4748\n",
      "[246]\tvalid_0's multi_logloss: 1.47459\n",
      "[247]\tvalid_0's multi_logloss: 1.47442\n",
      "[248]\tvalid_0's multi_logloss: 1.47431\n",
      "[249]\tvalid_0's multi_logloss: 1.47413\n",
      "[250]\tvalid_0's multi_logloss: 1.47394\n",
      "[251]\tvalid_0's multi_logloss: 1.47374\n",
      "[252]\tvalid_0's multi_logloss: 1.47351\n",
      "[253]\tvalid_0's multi_logloss: 1.47335\n",
      "[254]\tvalid_0's multi_logloss: 1.47323\n",
      "[255]\tvalid_0's multi_logloss: 1.47312\n",
      "[256]\tvalid_0's multi_logloss: 1.47293\n",
      "[257]\tvalid_0's multi_logloss: 1.47274\n",
      "[258]\tvalid_0's multi_logloss: 1.47261\n",
      "[259]\tvalid_0's multi_logloss: 1.4724\n",
      "[260]\tvalid_0's multi_logloss: 1.47221\n",
      "[261]\tvalid_0's multi_logloss: 1.47197\n",
      "[262]\tvalid_0's multi_logloss: 1.4718\n",
      "[263]\tvalid_0's multi_logloss: 1.47167\n",
      "[264]\tvalid_0's multi_logloss: 1.47156\n",
      "[265]\tvalid_0's multi_logloss: 1.47123\n",
      "[266]\tvalid_0's multi_logloss: 1.47103\n",
      "[267]\tvalid_0's multi_logloss: 1.47087\n",
      "[268]\tvalid_0's multi_logloss: 1.47073\n",
      "[269]\tvalid_0's multi_logloss: 1.47054\n",
      "[270]\tvalid_0's multi_logloss: 1.47044\n",
      "[271]\tvalid_0's multi_logloss: 1.47026\n",
      "[272]\tvalid_0's multi_logloss: 1.47011\n",
      "[273]\tvalid_0's multi_logloss: 1.46989\n",
      "[274]\tvalid_0's multi_logloss: 1.46978\n",
      "[275]\tvalid_0's multi_logloss: 1.46965\n",
      "[276]\tvalid_0's multi_logloss: 1.46949\n",
      "[277]\tvalid_0's multi_logloss: 1.46929\n",
      "[278]\tvalid_0's multi_logloss: 1.46915\n",
      "[279]\tvalid_0's multi_logloss: 1.46893\n",
      "[280]\tvalid_0's multi_logloss: 1.46883\n",
      "[281]\tvalid_0's multi_logloss: 1.46863\n",
      "[282]\tvalid_0's multi_logloss: 1.46849\n",
      "[283]\tvalid_0's multi_logloss: 1.46824\n",
      "[284]\tvalid_0's multi_logloss: 1.46811\n",
      "[285]\tvalid_0's multi_logloss: 1.46795\n",
      "[286]\tvalid_0's multi_logloss: 1.46777\n",
      "[287]\tvalid_0's multi_logloss: 1.46764\n",
      "[288]\tvalid_0's multi_logloss: 1.46745\n",
      "[289]\tvalid_0's multi_logloss: 1.4673\n",
      "[290]\tvalid_0's multi_logloss: 1.46723\n",
      "[291]\tvalid_0's multi_logloss: 1.46702\n",
      "[292]\tvalid_0's multi_logloss: 1.46687\n",
      "[293]\tvalid_0's multi_logloss: 1.46672\n",
      "[294]\tvalid_0's multi_logloss: 1.46659\n",
      "[295]\tvalid_0's multi_logloss: 1.46645\n",
      "[296]\tvalid_0's multi_logloss: 1.46628\n",
      "[297]\tvalid_0's multi_logloss: 1.46617\n",
      "[298]\tvalid_0's multi_logloss: 1.46596\n",
      "[299]\tvalid_0's multi_logloss: 1.46583\n",
      "[300]\tvalid_0's multi_logloss: 1.46563\n",
      "[301]\tvalid_0's multi_logloss: 1.46555\n",
      "[302]\tvalid_0's multi_logloss: 1.46543\n",
      "[303]\tvalid_0's multi_logloss: 1.46528\n",
      "[304]\tvalid_0's multi_logloss: 1.46516\n",
      "[305]\tvalid_0's multi_logloss: 1.46502\n",
      "[306]\tvalid_0's multi_logloss: 1.46498\n",
      "[307]\tvalid_0's multi_logloss: 1.46478\n",
      "[308]\tvalid_0's multi_logloss: 1.46461\n",
      "[309]\tvalid_0's multi_logloss: 1.46447\n",
      "[310]\tvalid_0's multi_logloss: 1.46432\n",
      "[311]\tvalid_0's multi_logloss: 1.46418\n",
      "[312]\tvalid_0's multi_logloss: 1.46399\n",
      "[313]\tvalid_0's multi_logloss: 1.46386\n",
      "[314]\tvalid_0's multi_logloss: 1.46371\n",
      "[315]\tvalid_0's multi_logloss: 1.46349\n",
      "[316]\tvalid_0's multi_logloss: 1.46337\n",
      "[317]\tvalid_0's multi_logloss: 1.46313\n",
      "[318]\tvalid_0's multi_logloss: 1.46291\n",
      "[319]\tvalid_0's multi_logloss: 1.46271\n",
      "[320]\tvalid_0's multi_logloss: 1.46261\n",
      "[321]\tvalid_0's multi_logloss: 1.46241\n",
      "[322]\tvalid_0's multi_logloss: 1.46225\n",
      "[323]\tvalid_0's multi_logloss: 1.46204\n",
      "[324]\tvalid_0's multi_logloss: 1.46192\n",
      "[325]\tvalid_0's multi_logloss: 1.4618\n",
      "[326]\tvalid_0's multi_logloss: 1.46167\n",
      "[327]\tvalid_0's multi_logloss: 1.46154\n",
      "[328]\tvalid_0's multi_logloss: 1.46121\n",
      "[329]\tvalid_0's multi_logloss: 1.46113\n",
      "[330]\tvalid_0's multi_logloss: 1.46098\n",
      "[331]\tvalid_0's multi_logloss: 1.46087\n",
      "[332]\tvalid_0's multi_logloss: 1.46068\n",
      "[333]\tvalid_0's multi_logloss: 1.46056\n",
      "[334]\tvalid_0's multi_logloss: 1.46043\n",
      "[335]\tvalid_0's multi_logloss: 1.46022\n",
      "[336]\tvalid_0's multi_logloss: 1.46011\n",
      "[337]\tvalid_0's multi_logloss: 1.46\n",
      "[338]\tvalid_0's multi_logloss: 1.45989\n",
      "[339]\tvalid_0's multi_logloss: 1.45973\n",
      "[340]\tvalid_0's multi_logloss: 1.45961\n",
      "[341]\tvalid_0's multi_logloss: 1.4595\n",
      "[342]\tvalid_0's multi_logloss: 1.45929\n",
      "[343]\tvalid_0's multi_logloss: 1.45913\n",
      "[344]\tvalid_0's multi_logloss: 1.45902\n",
      "[345]\tvalid_0's multi_logloss: 1.45886\n",
      "[346]\tvalid_0's multi_logloss: 1.45865\n",
      "[347]\tvalid_0's multi_logloss: 1.45849\n",
      "[348]\tvalid_0's multi_logloss: 1.45842\n",
      "[349]\tvalid_0's multi_logloss: 1.45835\n",
      "[350]\tvalid_0's multi_logloss: 1.45816\n",
      "[351]\tvalid_0's multi_logloss: 1.45787\n",
      "[352]\tvalid_0's multi_logloss: 1.45776\n",
      "[353]\tvalid_0's multi_logloss: 1.45768\n",
      "[354]\tvalid_0's multi_logloss: 1.45758\n",
      "[355]\tvalid_0's multi_logloss: 1.45742\n",
      "[356]\tvalid_0's multi_logloss: 1.45727\n",
      "[357]\tvalid_0's multi_logloss: 1.45716\n",
      "[358]\tvalid_0's multi_logloss: 1.45708\n",
      "[359]\tvalid_0's multi_logloss: 1.45699\n",
      "[360]\tvalid_0's multi_logloss: 1.45683\n",
      "[361]\tvalid_0's multi_logloss: 1.45668\n",
      "[362]\tvalid_0's multi_logloss: 1.45653\n",
      "[363]\tvalid_0's multi_logloss: 1.4564\n",
      "[364]\tvalid_0's multi_logloss: 1.45624\n",
      "[365]\tvalid_0's multi_logloss: 1.45611\n",
      "[366]\tvalid_0's multi_logloss: 1.45589\n",
      "[367]\tvalid_0's multi_logloss: 1.45578\n",
      "[368]\tvalid_0's multi_logloss: 1.45564\n",
      "[369]\tvalid_0's multi_logloss: 1.45556\n",
      "[370]\tvalid_0's multi_logloss: 1.45555\n",
      "[371]\tvalid_0's multi_logloss: 1.45541\n",
      "[372]\tvalid_0's multi_logloss: 1.45527\n",
      "[373]\tvalid_0's multi_logloss: 1.45512\n",
      "[374]\tvalid_0's multi_logloss: 1.45497\n",
      "[375]\tvalid_0's multi_logloss: 1.45492\n",
      "[376]\tvalid_0's multi_logloss: 1.45478\n",
      "[377]\tvalid_0's multi_logloss: 1.45464\n",
      "[378]\tvalid_0's multi_logloss: 1.45446\n",
      "[379]\tvalid_0's multi_logloss: 1.45437\n",
      "[380]\tvalid_0's multi_logloss: 1.45428\n",
      "[381]\tvalid_0's multi_logloss: 1.45419\n",
      "[382]\tvalid_0's multi_logloss: 1.45397\n",
      "[383]\tvalid_0's multi_logloss: 1.45385\n",
      "[384]\tvalid_0's multi_logloss: 1.45371\n",
      "[385]\tvalid_0's multi_logloss: 1.45361\n",
      "[386]\tvalid_0's multi_logloss: 1.45348\n",
      "[387]\tvalid_0's multi_logloss: 1.45341\n",
      "[388]\tvalid_0's multi_logloss: 1.4533\n",
      "[389]\tvalid_0's multi_logloss: 1.45324\n",
      "[390]\tvalid_0's multi_logloss: 1.45317\n",
      "[391]\tvalid_0's multi_logloss: 1.45304\n",
      "[392]\tvalid_0's multi_logloss: 1.4529\n",
      "[393]\tvalid_0's multi_logloss: 1.45279\n",
      "[394]\tvalid_0's multi_logloss: 1.45258\n",
      "[395]\tvalid_0's multi_logloss: 1.45242\n",
      "[396]\tvalid_0's multi_logloss: 1.45227\n",
      "[397]\tvalid_0's multi_logloss: 1.45216\n",
      "[398]\tvalid_0's multi_logloss: 1.45203\n",
      "[399]\tvalid_0's multi_logloss: 1.45199\n",
      "[400]\tvalid_0's multi_logloss: 1.45192\n",
      "[401]\tvalid_0's multi_logloss: 1.45182\n",
      "[402]\tvalid_0's multi_logloss: 1.45162\n",
      "[403]\tvalid_0's multi_logloss: 1.45149\n",
      "[404]\tvalid_0's multi_logloss: 1.45143\n",
      "[405]\tvalid_0's multi_logloss: 1.45132\n",
      "[406]\tvalid_0's multi_logloss: 1.45119\n",
      "[407]\tvalid_0's multi_logloss: 1.45108\n",
      "[408]\tvalid_0's multi_logloss: 1.45091\n",
      "[409]\tvalid_0's multi_logloss: 1.45083\n",
      "[410]\tvalid_0's multi_logloss: 1.45068\n",
      "[411]\tvalid_0's multi_logloss: 1.45058\n",
      "[412]\tvalid_0's multi_logloss: 1.45052\n",
      "[413]\tvalid_0's multi_logloss: 1.45028\n",
      "[414]\tvalid_0's multi_logloss: 1.45018\n",
      "[415]\tvalid_0's multi_logloss: 1.45017\n",
      "[416]\tvalid_0's multi_logloss: 1.45008\n",
      "[417]\tvalid_0's multi_logloss: 1.45005\n",
      "[418]\tvalid_0's multi_logloss: 1.44992\n",
      "[419]\tvalid_0's multi_logloss: 1.44978\n",
      "[420]\tvalid_0's multi_logloss: 1.44967\n",
      "[421]\tvalid_0's multi_logloss: 1.44958\n",
      "[422]\tvalid_0's multi_logloss: 1.44952\n",
      "[423]\tvalid_0's multi_logloss: 1.44939\n",
      "[424]\tvalid_0's multi_logloss: 1.44927\n",
      "[425]\tvalid_0's multi_logloss: 1.44921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[426]\tvalid_0's multi_logloss: 1.44913\n",
      "[427]\tvalid_0's multi_logloss: 1.4491\n",
      "[428]\tvalid_0's multi_logloss: 1.44902\n",
      "[429]\tvalid_0's multi_logloss: 1.44893\n",
      "[430]\tvalid_0's multi_logloss: 1.44881\n",
      "[431]\tvalid_0's multi_logloss: 1.44867\n",
      "[432]\tvalid_0's multi_logloss: 1.44857\n",
      "[433]\tvalid_0's multi_logloss: 1.44847\n",
      "[434]\tvalid_0's multi_logloss: 1.44833\n",
      "[435]\tvalid_0's multi_logloss: 1.44819\n",
      "[436]\tvalid_0's multi_logloss: 1.44813\n",
      "[437]\tvalid_0's multi_logloss: 1.44792\n",
      "[438]\tvalid_0's multi_logloss: 1.44779\n",
      "[439]\tvalid_0's multi_logloss: 1.44776\n",
      "[440]\tvalid_0's multi_logloss: 1.4476\n",
      "[441]\tvalid_0's multi_logloss: 1.44754\n",
      "[442]\tvalid_0's multi_logloss: 1.44741\n",
      "[443]\tvalid_0's multi_logloss: 1.44731\n",
      "[444]\tvalid_0's multi_logloss: 1.44724\n",
      "[445]\tvalid_0's multi_logloss: 1.44715\n",
      "[446]\tvalid_0's multi_logloss: 1.44714\n",
      "[447]\tvalid_0's multi_logloss: 1.44704\n",
      "[448]\tvalid_0's multi_logloss: 1.44695\n",
      "[449]\tvalid_0's multi_logloss: 1.44682\n",
      "[450]\tvalid_0's multi_logloss: 1.44663\n",
      "[451]\tvalid_0's multi_logloss: 1.4465\n",
      "[452]\tvalid_0's multi_logloss: 1.4464\n",
      "[453]\tvalid_0's multi_logloss: 1.44632\n",
      "[454]\tvalid_0's multi_logloss: 1.44625\n",
      "[455]\tvalid_0's multi_logloss: 1.44616\n",
      "[456]\tvalid_0's multi_logloss: 1.44599\n",
      "[457]\tvalid_0's multi_logloss: 1.44594\n",
      "[458]\tvalid_0's multi_logloss: 1.44578\n",
      "[459]\tvalid_0's multi_logloss: 1.44571\n",
      "[460]\tvalid_0's multi_logloss: 1.44564\n",
      "[461]\tvalid_0's multi_logloss: 1.4455\n",
      "[462]\tvalid_0's multi_logloss: 1.44533\n",
      "[463]\tvalid_0's multi_logloss: 1.44524\n",
      "[464]\tvalid_0's multi_logloss: 1.44517\n",
      "[465]\tvalid_0's multi_logloss: 1.4451\n",
      "[466]\tvalid_0's multi_logloss: 1.44497\n",
      "[467]\tvalid_0's multi_logloss: 1.44488\n",
      "[468]\tvalid_0's multi_logloss: 1.44476\n",
      "[469]\tvalid_0's multi_logloss: 1.4447\n",
      "[470]\tvalid_0's multi_logloss: 1.44461\n",
      "[471]\tvalid_0's multi_logloss: 1.4445\n",
      "[472]\tvalid_0's multi_logloss: 1.44427\n",
      "[473]\tvalid_0's multi_logloss: 1.44418\n",
      "[474]\tvalid_0's multi_logloss: 1.44414\n",
      "[475]\tvalid_0's multi_logloss: 1.44395\n",
      "[476]\tvalid_0's multi_logloss: 1.44381\n",
      "[477]\tvalid_0's multi_logloss: 1.44368\n",
      "[478]\tvalid_0's multi_logloss: 1.44359\n",
      "[479]\tvalid_0's multi_logloss: 1.44351\n",
      "[480]\tvalid_0's multi_logloss: 1.44345\n",
      "[481]\tvalid_0's multi_logloss: 1.44338\n",
      "[482]\tvalid_0's multi_logloss: 1.44332\n",
      "[483]\tvalid_0's multi_logloss: 1.44317\n",
      "[484]\tvalid_0's multi_logloss: 1.44312\n",
      "[485]\tvalid_0's multi_logloss: 1.44295\n",
      "[486]\tvalid_0's multi_logloss: 1.44282\n",
      "[487]\tvalid_0's multi_logloss: 1.44278\n",
      "[488]\tvalid_0's multi_logloss: 1.44266\n",
      "[489]\tvalid_0's multi_logloss: 1.4426\n",
      "[490]\tvalid_0's multi_logloss: 1.44257\n",
      "[491]\tvalid_0's multi_logloss: 1.44255\n",
      "[492]\tvalid_0's multi_logloss: 1.44246\n",
      "[493]\tvalid_0's multi_logloss: 1.44237\n",
      "[494]\tvalid_0's multi_logloss: 1.44223\n",
      "[495]\tvalid_0's multi_logloss: 1.44213\n",
      "[496]\tvalid_0's multi_logloss: 1.44202\n",
      "[497]\tvalid_0's multi_logloss: 1.44199\n",
      "[498]\tvalid_0's multi_logloss: 1.44187\n",
      "[499]\tvalid_0's multi_logloss: 1.44179\n",
      "[500]\tvalid_0's multi_logloss: 1.4417\n",
      "[501]\tvalid_0's multi_logloss: 1.44158\n",
      "[502]\tvalid_0's multi_logloss: 1.44155\n",
      "[503]\tvalid_0's multi_logloss: 1.44137\n",
      "[504]\tvalid_0's multi_logloss: 1.44126\n",
      "[505]\tvalid_0's multi_logloss: 1.44111\n",
      "[506]\tvalid_0's multi_logloss: 1.44103\n",
      "[507]\tvalid_0's multi_logloss: 1.44094\n",
      "[508]\tvalid_0's multi_logloss: 1.44094\n",
      "[509]\tvalid_0's multi_logloss: 1.44071\n",
      "[510]\tvalid_0's multi_logloss: 1.44066\n",
      "[511]\tvalid_0's multi_logloss: 1.44057\n",
      "[512]\tvalid_0's multi_logloss: 1.44054\n",
      "[513]\tvalid_0's multi_logloss: 1.44041\n",
      "[514]\tvalid_0's multi_logloss: 1.44031\n",
      "[515]\tvalid_0's multi_logloss: 1.44032\n",
      "[516]\tvalid_0's multi_logloss: 1.44014\n",
      "[517]\tvalid_0's multi_logloss: 1.44009\n",
      "[518]\tvalid_0's multi_logloss: 1.44001\n",
      "[519]\tvalid_0's multi_logloss: 1.43997\n",
      "[520]\tvalid_0's multi_logloss: 1.43988\n",
      "[521]\tvalid_0's multi_logloss: 1.43979\n",
      "[522]\tvalid_0's multi_logloss: 1.43968\n",
      "[523]\tvalid_0's multi_logloss: 1.43956\n",
      "[524]\tvalid_0's multi_logloss: 1.43945\n",
      "[525]\tvalid_0's multi_logloss: 1.43938\n",
      "[526]\tvalid_0's multi_logloss: 1.43933\n",
      "[527]\tvalid_0's multi_logloss: 1.43926\n",
      "[528]\tvalid_0's multi_logloss: 1.43918\n",
      "[529]\tvalid_0's multi_logloss: 1.43911\n",
      "[530]\tvalid_0's multi_logloss: 1.43906\n",
      "[531]\tvalid_0's multi_logloss: 1.43899\n",
      "[532]\tvalid_0's multi_logloss: 1.4389\n",
      "[533]\tvalid_0's multi_logloss: 1.43888\n",
      "[534]\tvalid_0's multi_logloss: 1.43875\n",
      "[535]\tvalid_0's multi_logloss: 1.43868\n",
      "[536]\tvalid_0's multi_logloss: 1.43858\n",
      "[537]\tvalid_0's multi_logloss: 1.43848\n",
      "[538]\tvalid_0's multi_logloss: 1.43837\n",
      "[539]\tvalid_0's multi_logloss: 1.43824\n",
      "[540]\tvalid_0's multi_logloss: 1.43815\n",
      "[541]\tvalid_0's multi_logloss: 1.43804\n",
      "[542]\tvalid_0's multi_logloss: 1.43795\n",
      "[543]\tvalid_0's multi_logloss: 1.4379\n",
      "[544]\tvalid_0's multi_logloss: 1.43781\n",
      "[545]\tvalid_0's multi_logloss: 1.43776\n",
      "[546]\tvalid_0's multi_logloss: 1.43766\n",
      "[547]\tvalid_0's multi_logloss: 1.43762\n",
      "[548]\tvalid_0's multi_logloss: 1.43758\n",
      "[549]\tvalid_0's multi_logloss: 1.4375\n",
      "[550]\tvalid_0's multi_logloss: 1.43737\n",
      "[551]\tvalid_0's multi_logloss: 1.43728\n",
      "[552]\tvalid_0's multi_logloss: 1.43726\n",
      "[553]\tvalid_0's multi_logloss: 1.43708\n",
      "[554]\tvalid_0's multi_logloss: 1.43708\n",
      "[555]\tvalid_0's multi_logloss: 1.43705\n",
      "[556]\tvalid_0's multi_logloss: 1.43699\n",
      "[557]\tvalid_0's multi_logloss: 1.43691\n",
      "[558]\tvalid_0's multi_logloss: 1.43681\n",
      "[559]\tvalid_0's multi_logloss: 1.43672\n",
      "[560]\tvalid_0's multi_logloss: 1.43668\n",
      "[561]\tvalid_0's multi_logloss: 1.4366\n",
      "[562]\tvalid_0's multi_logloss: 1.4365\n",
      "[563]\tvalid_0's multi_logloss: 1.43649\n",
      "[564]\tvalid_0's multi_logloss: 1.43634\n",
      "[565]\tvalid_0's multi_logloss: 1.43627\n",
      "[566]\tvalid_0's multi_logloss: 1.43626\n",
      "[567]\tvalid_0's multi_logloss: 1.43621\n",
      "[568]\tvalid_0's multi_logloss: 1.43611\n",
      "[569]\tvalid_0's multi_logloss: 1.43604\n",
      "[570]\tvalid_0's multi_logloss: 1.43593\n",
      "[571]\tvalid_0's multi_logloss: 1.43589\n",
      "[572]\tvalid_0's multi_logloss: 1.43577\n",
      "[573]\tvalid_0's multi_logloss: 1.43564\n",
      "[574]\tvalid_0's multi_logloss: 1.43553\n",
      "[575]\tvalid_0's multi_logloss: 1.43546\n",
      "[576]\tvalid_0's multi_logloss: 1.43542\n",
      "[577]\tvalid_0's multi_logloss: 1.43539\n",
      "[578]\tvalid_0's multi_logloss: 1.43538\n",
      "[579]\tvalid_0's multi_logloss: 1.43527\n",
      "[580]\tvalid_0's multi_logloss: 1.43526\n",
      "[581]\tvalid_0's multi_logloss: 1.4352\n",
      "[582]\tvalid_0's multi_logloss: 1.43515\n",
      "[583]\tvalid_0's multi_logloss: 1.43511\n",
      "[584]\tvalid_0's multi_logloss: 1.43504\n",
      "[585]\tvalid_0's multi_logloss: 1.43499\n",
      "[586]\tvalid_0's multi_logloss: 1.43486\n",
      "[587]\tvalid_0's multi_logloss: 1.4348\n",
      "[588]\tvalid_0's multi_logloss: 1.43474\n",
      "[589]\tvalid_0's multi_logloss: 1.43473\n",
      "[590]\tvalid_0's multi_logloss: 1.4346\n",
      "[591]\tvalid_0's multi_logloss: 1.43451\n",
      "[592]\tvalid_0's multi_logloss: 1.43447\n",
      "[593]\tvalid_0's multi_logloss: 1.43443\n",
      "[594]\tvalid_0's multi_logloss: 1.43435\n",
      "[595]\tvalid_0's multi_logloss: 1.43426\n",
      "[596]\tvalid_0's multi_logloss: 1.43419\n",
      "[597]\tvalid_0's multi_logloss: 1.43412\n",
      "[598]\tvalid_0's multi_logloss: 1.43406\n",
      "[599]\tvalid_0's multi_logloss: 1.43401\n",
      "[600]\tvalid_0's multi_logloss: 1.43394\n",
      "[601]\tvalid_0's multi_logloss: 1.43386\n",
      "[602]\tvalid_0's multi_logloss: 1.43372\n",
      "[603]\tvalid_0's multi_logloss: 1.43367\n",
      "[604]\tvalid_0's multi_logloss: 1.43361\n",
      "[605]\tvalid_0's multi_logloss: 1.43356\n",
      "[606]\tvalid_0's multi_logloss: 1.43352\n",
      "[607]\tvalid_0's multi_logloss: 1.43341\n",
      "[608]\tvalid_0's multi_logloss: 1.43336\n",
      "[609]\tvalid_0's multi_logloss: 1.43331\n",
      "[610]\tvalid_0's multi_logloss: 1.43327\n",
      "[611]\tvalid_0's multi_logloss: 1.43326\n",
      "[612]\tvalid_0's multi_logloss: 1.43319\n",
      "[613]\tvalid_0's multi_logloss: 1.43314\n",
      "[614]\tvalid_0's multi_logloss: 1.43307\n",
      "[615]\tvalid_0's multi_logloss: 1.433\n",
      "[616]\tvalid_0's multi_logloss: 1.43305\n",
      "[617]\tvalid_0's multi_logloss: 1.43297\n",
      "[618]\tvalid_0's multi_logloss: 1.43289\n",
      "[619]\tvalid_0's multi_logloss: 1.43282\n",
      "[620]\tvalid_0's multi_logloss: 1.4328\n",
      "[621]\tvalid_0's multi_logloss: 1.43269\n",
      "[622]\tvalid_0's multi_logloss: 1.43268\n",
      "[623]\tvalid_0's multi_logloss: 1.43258\n",
      "[624]\tvalid_0's multi_logloss: 1.43246\n",
      "[625]\tvalid_0's multi_logloss: 1.43237\n",
      "[626]\tvalid_0's multi_logloss: 1.43233\n",
      "[627]\tvalid_0's multi_logloss: 1.43229\n",
      "[628]\tvalid_0's multi_logloss: 1.43219\n",
      "[629]\tvalid_0's multi_logloss: 1.4321\n",
      "[630]\tvalid_0's multi_logloss: 1.43204\n",
      "[631]\tvalid_0's multi_logloss: 1.43194\n",
      "[632]\tvalid_0's multi_logloss: 1.4319\n",
      "[633]\tvalid_0's multi_logloss: 1.4318\n",
      "[634]\tvalid_0's multi_logloss: 1.43174\n",
      "[635]\tvalid_0's multi_logloss: 1.43166\n",
      "[636]\tvalid_0's multi_logloss: 1.43165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[637]\tvalid_0's multi_logloss: 1.43162\n",
      "[638]\tvalid_0's multi_logloss: 1.43154\n",
      "[639]\tvalid_0's multi_logloss: 1.43146\n",
      "[640]\tvalid_0's multi_logloss: 1.43138\n",
      "[641]\tvalid_0's multi_logloss: 1.43135\n",
      "[642]\tvalid_0's multi_logloss: 1.43127\n",
      "[643]\tvalid_0's multi_logloss: 1.4312\n",
      "[644]\tvalid_0's multi_logloss: 1.43117\n",
      "[645]\tvalid_0's multi_logloss: 1.4311\n",
      "[646]\tvalid_0's multi_logloss: 1.43108\n",
      "[647]\tvalid_0's multi_logloss: 1.43105\n",
      "[648]\tvalid_0's multi_logloss: 1.43094\n",
      "[649]\tvalid_0's multi_logloss: 1.43093\n",
      "[650]\tvalid_0's multi_logloss: 1.43093\n",
      "[651]\tvalid_0's multi_logloss: 1.43083\n",
      "[652]\tvalid_0's multi_logloss: 1.43071\n",
      "[653]\tvalid_0's multi_logloss: 1.4307\n",
      "[654]\tvalid_0's multi_logloss: 1.43064\n",
      "[655]\tvalid_0's multi_logloss: 1.43057\n",
      "[656]\tvalid_0's multi_logloss: 1.43056\n",
      "[657]\tvalid_0's multi_logloss: 1.4305\n",
      "[658]\tvalid_0's multi_logloss: 1.43044\n",
      "[659]\tvalid_0's multi_logloss: 1.43037\n",
      "[660]\tvalid_0's multi_logloss: 1.43031\n",
      "[661]\tvalid_0's multi_logloss: 1.43017\n",
      "[662]\tvalid_0's multi_logloss: 1.4301\n",
      "[663]\tvalid_0's multi_logloss: 1.43005\n",
      "[664]\tvalid_0's multi_logloss: 1.43\n",
      "[665]\tvalid_0's multi_logloss: 1.42987\n",
      "[666]\tvalid_0's multi_logloss: 1.42984\n",
      "[667]\tvalid_0's multi_logloss: 1.42975\n",
      "[668]\tvalid_0's multi_logloss: 1.4297\n",
      "[669]\tvalid_0's multi_logloss: 1.42968\n",
      "[670]\tvalid_0's multi_logloss: 1.42955\n",
      "[671]\tvalid_0's multi_logloss: 1.42949\n",
      "[672]\tvalid_0's multi_logloss: 1.42945\n",
      "[673]\tvalid_0's multi_logloss: 1.42939\n",
      "[674]\tvalid_0's multi_logloss: 1.42933\n",
      "[675]\tvalid_0's multi_logloss: 1.42929\n",
      "[676]\tvalid_0's multi_logloss: 1.42927\n",
      "[677]\tvalid_0's multi_logloss: 1.42921\n",
      "[678]\tvalid_0's multi_logloss: 1.42916\n",
      "[679]\tvalid_0's multi_logloss: 1.42907\n",
      "[680]\tvalid_0's multi_logloss: 1.42899\n",
      "[681]\tvalid_0's multi_logloss: 1.42893\n",
      "[682]\tvalid_0's multi_logloss: 1.42889\n",
      "[683]\tvalid_0's multi_logloss: 1.42876\n",
      "[684]\tvalid_0's multi_logloss: 1.42872\n",
      "[685]\tvalid_0's multi_logloss: 1.42864\n",
      "[686]\tvalid_0's multi_logloss: 1.42861\n",
      "[687]\tvalid_0's multi_logloss: 1.42856\n",
      "[688]\tvalid_0's multi_logloss: 1.42851\n",
      "[689]\tvalid_0's multi_logloss: 1.42843\n",
      "[690]\tvalid_0's multi_logloss: 1.42826\n",
      "[691]\tvalid_0's multi_logloss: 1.42815\n",
      "[692]\tvalid_0's multi_logloss: 1.42815\n",
      "[693]\tvalid_0's multi_logloss: 1.42815\n",
      "[694]\tvalid_0's multi_logloss: 1.42814\n",
      "[695]\tvalid_0's multi_logloss: 1.42819\n",
      "[696]\tvalid_0's multi_logloss: 1.42808\n",
      "[697]\tvalid_0's multi_logloss: 1.42798\n",
      "[698]\tvalid_0's multi_logloss: 1.4279\n",
      "[699]\tvalid_0's multi_logloss: 1.42784\n",
      "[700]\tvalid_0's multi_logloss: 1.42771\n",
      "[701]\tvalid_0's multi_logloss: 1.42767\n",
      "[702]\tvalid_0's multi_logloss: 1.42761\n",
      "[703]\tvalid_0's multi_logloss: 1.4276\n",
      "[704]\tvalid_0's multi_logloss: 1.42753\n",
      "[705]\tvalid_0's multi_logloss: 1.42745\n",
      "[706]\tvalid_0's multi_logloss: 1.42732\n",
      "[707]\tvalid_0's multi_logloss: 1.42723\n",
      "[708]\tvalid_0's multi_logloss: 1.42723\n",
      "[709]\tvalid_0's multi_logloss: 1.4272\n",
      "[710]\tvalid_0's multi_logloss: 1.42718\n",
      "[711]\tvalid_0's multi_logloss: 1.42707\n",
      "[712]\tvalid_0's multi_logloss: 1.42698\n",
      "[713]\tvalid_0's multi_logloss: 1.4269\n",
      "[714]\tvalid_0's multi_logloss: 1.42683\n",
      "[715]\tvalid_0's multi_logloss: 1.42673\n",
      "[716]\tvalid_0's multi_logloss: 1.42669\n",
      "[717]\tvalid_0's multi_logloss: 1.4266\n",
      "[718]\tvalid_0's multi_logloss: 1.42655\n",
      "[719]\tvalid_0's multi_logloss: 1.42649\n",
      "[720]\tvalid_0's multi_logloss: 1.42644\n",
      "[721]\tvalid_0's multi_logloss: 1.42644\n",
      "[722]\tvalid_0's multi_logloss: 1.42633\n",
      "[723]\tvalid_0's multi_logloss: 1.42629\n",
      "[724]\tvalid_0's multi_logloss: 1.42624\n",
      "[725]\tvalid_0's multi_logloss: 1.42619\n",
      "[726]\tvalid_0's multi_logloss: 1.42612\n",
      "[727]\tvalid_0's multi_logloss: 1.42604\n",
      "[728]\tvalid_0's multi_logloss: 1.42597\n",
      "[729]\tvalid_0's multi_logloss: 1.42594\n",
      "[730]\tvalid_0's multi_logloss: 1.42595\n",
      "[731]\tvalid_0's multi_logloss: 1.42586\n",
      "[732]\tvalid_0's multi_logloss: 1.42573\n",
      "[733]\tvalid_0's multi_logloss: 1.42565\n",
      "[734]\tvalid_0's multi_logloss: 1.42548\n",
      "[735]\tvalid_0's multi_logloss: 1.42543\n",
      "[736]\tvalid_0's multi_logloss: 1.42539\n",
      "[737]\tvalid_0's multi_logloss: 1.42539\n",
      "[738]\tvalid_0's multi_logloss: 1.42534\n",
      "[739]\tvalid_0's multi_logloss: 1.42524\n",
      "[740]\tvalid_0's multi_logloss: 1.42514\n",
      "[741]\tvalid_0's multi_logloss: 1.42511\n",
      "[742]\tvalid_0's multi_logloss: 1.42503\n",
      "[743]\tvalid_0's multi_logloss: 1.42499\n",
      "[744]\tvalid_0's multi_logloss: 1.42498\n",
      "[745]\tvalid_0's multi_logloss: 1.42492\n",
      "[746]\tvalid_0's multi_logloss: 1.42488\n",
      "[747]\tvalid_0's multi_logloss: 1.4248\n",
      "[748]\tvalid_0's multi_logloss: 1.42472\n",
      "[749]\tvalid_0's multi_logloss: 1.42462\n",
      "[750]\tvalid_0's multi_logloss: 1.42451\n",
      "[751]\tvalid_0's multi_logloss: 1.42441\n",
      "[752]\tvalid_0's multi_logloss: 1.42439\n",
      "[753]\tvalid_0's multi_logloss: 1.42424\n",
      "[754]\tvalid_0's multi_logloss: 1.42416\n",
      "[755]\tvalid_0's multi_logloss: 1.42406\n",
      "[756]\tvalid_0's multi_logloss: 1.42408\n",
      "[757]\tvalid_0's multi_logloss: 1.42407\n",
      "[758]\tvalid_0's multi_logloss: 1.42401\n",
      "[759]\tvalid_0's multi_logloss: 1.42393\n",
      "[760]\tvalid_0's multi_logloss: 1.42391\n",
      "[761]\tvalid_0's multi_logloss: 1.42392\n",
      "[762]\tvalid_0's multi_logloss: 1.42385\n",
      "[763]\tvalid_0's multi_logloss: 1.42381\n",
      "[764]\tvalid_0's multi_logloss: 1.42375\n",
      "[765]\tvalid_0's multi_logloss: 1.42369\n",
      "[766]\tvalid_0's multi_logloss: 1.42364\n",
      "[767]\tvalid_0's multi_logloss: 1.42353\n",
      "[768]\tvalid_0's multi_logloss: 1.42355\n",
      "[769]\tvalid_0's multi_logloss: 1.42347\n",
      "[770]\tvalid_0's multi_logloss: 1.42346\n",
      "[771]\tvalid_0's multi_logloss: 1.4234\n",
      "[772]\tvalid_0's multi_logloss: 1.42328\n",
      "[773]\tvalid_0's multi_logloss: 1.42322\n",
      "[774]\tvalid_0's multi_logloss: 1.42323\n",
      "[775]\tvalid_0's multi_logloss: 1.42322\n",
      "[776]\tvalid_0's multi_logloss: 1.42315\n",
      "[777]\tvalid_0's multi_logloss: 1.42306\n",
      "[778]\tvalid_0's multi_logloss: 1.42302\n",
      "[779]\tvalid_0's multi_logloss: 1.42301\n",
      "[780]\tvalid_0's multi_logloss: 1.42296\n",
      "[781]\tvalid_0's multi_logloss: 1.42294\n",
      "[782]\tvalid_0's multi_logloss: 1.42285\n",
      "[783]\tvalid_0's multi_logloss: 1.42277\n",
      "[784]\tvalid_0's multi_logloss: 1.42271\n",
      "[785]\tvalid_0's multi_logloss: 1.42266\n",
      "[786]\tvalid_0's multi_logloss: 1.42259\n",
      "[787]\tvalid_0's multi_logloss: 1.42255\n",
      "[788]\tvalid_0's multi_logloss: 1.42251\n",
      "[789]\tvalid_0's multi_logloss: 1.42248\n",
      "[790]\tvalid_0's multi_logloss: 1.42246\n",
      "[791]\tvalid_0's multi_logloss: 1.42247\n",
      "[792]\tvalid_0's multi_logloss: 1.42242\n",
      "[793]\tvalid_0's multi_logloss: 1.42234\n",
      "[794]\tvalid_0's multi_logloss: 1.42225\n",
      "[795]\tvalid_0's multi_logloss: 1.42218\n",
      "[796]\tvalid_0's multi_logloss: 1.42212\n",
      "[797]\tvalid_0's multi_logloss: 1.42213\n",
      "[798]\tvalid_0's multi_logloss: 1.42209\n",
      "[799]\tvalid_0's multi_logloss: 1.42205\n",
      "[800]\tvalid_0's multi_logloss: 1.42199\n",
      "[801]\tvalid_0's multi_logloss: 1.42186\n",
      "[802]\tvalid_0's multi_logloss: 1.42186\n",
      "[803]\tvalid_0's multi_logloss: 1.42177\n",
      "[804]\tvalid_0's multi_logloss: 1.4217\n",
      "[805]\tvalid_0's multi_logloss: 1.4216\n",
      "[806]\tvalid_0's multi_logloss: 1.42155\n",
      "[807]\tvalid_0's multi_logloss: 1.42152\n",
      "[808]\tvalid_0's multi_logloss: 1.42151\n",
      "[809]\tvalid_0's multi_logloss: 1.42139\n",
      "[810]\tvalid_0's multi_logloss: 1.42134\n",
      "[811]\tvalid_0's multi_logloss: 1.42127\n",
      "[812]\tvalid_0's multi_logloss: 1.42127\n",
      "[813]\tvalid_0's multi_logloss: 1.42124\n",
      "[814]\tvalid_0's multi_logloss: 1.42121\n",
      "[815]\tvalid_0's multi_logloss: 1.42118\n",
      "[816]\tvalid_0's multi_logloss: 1.42112\n",
      "[817]\tvalid_0's multi_logloss: 1.42105\n",
      "[818]\tvalid_0's multi_logloss: 1.42098\n",
      "[819]\tvalid_0's multi_logloss: 1.42092\n",
      "[820]\tvalid_0's multi_logloss: 1.42085\n",
      "[821]\tvalid_0's multi_logloss: 1.42086\n",
      "[822]\tvalid_0's multi_logloss: 1.42078\n",
      "[823]\tvalid_0's multi_logloss: 1.42074\n",
      "[824]\tvalid_0's multi_logloss: 1.4207\n",
      "[825]\tvalid_0's multi_logloss: 1.42068\n",
      "[826]\tvalid_0's multi_logloss: 1.42064\n",
      "[827]\tvalid_0's multi_logloss: 1.42061\n",
      "[828]\tvalid_0's multi_logloss: 1.42056\n",
      "[829]\tvalid_0's multi_logloss: 1.42055\n",
      "[830]\tvalid_0's multi_logloss: 1.42049\n",
      "[831]\tvalid_0's multi_logloss: 1.42044\n",
      "[832]\tvalid_0's multi_logloss: 1.42039\n",
      "[833]\tvalid_0's multi_logloss: 1.42043\n",
      "[834]\tvalid_0's multi_logloss: 1.42037\n",
      "[835]\tvalid_0's multi_logloss: 1.42033\n",
      "[836]\tvalid_0's multi_logloss: 1.42023\n",
      "[837]\tvalid_0's multi_logloss: 1.42015\n",
      "[838]\tvalid_0's multi_logloss: 1.42013\n",
      "[839]\tvalid_0's multi_logloss: 1.42008\n",
      "[840]\tvalid_0's multi_logloss: 1.42005\n",
      "[841]\tvalid_0's multi_logloss: 1.42002\n",
      "[842]\tvalid_0's multi_logloss: 1.42\n",
      "[843]\tvalid_0's multi_logloss: 1.41998\n",
      "[844]\tvalid_0's multi_logloss: 1.4199\n",
      "[845]\tvalid_0's multi_logloss: 1.41988\n",
      "[846]\tvalid_0's multi_logloss: 1.41982\n",
      "[847]\tvalid_0's multi_logloss: 1.4198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[848]\tvalid_0's multi_logloss: 1.41969\n",
      "[849]\tvalid_0's multi_logloss: 1.41962\n",
      "[850]\tvalid_0's multi_logloss: 1.41966\n",
      "[851]\tvalid_0's multi_logloss: 1.41962\n",
      "[852]\tvalid_0's multi_logloss: 1.41957\n",
      "[853]\tvalid_0's multi_logloss: 1.41951\n",
      "[854]\tvalid_0's multi_logloss: 1.41944\n",
      "[855]\tvalid_0's multi_logloss: 1.41949\n",
      "[856]\tvalid_0's multi_logloss: 1.41943\n",
      "[857]\tvalid_0's multi_logloss: 1.41937\n",
      "[858]\tvalid_0's multi_logloss: 1.41933\n",
      "[859]\tvalid_0's multi_logloss: 1.41924\n",
      "[860]\tvalid_0's multi_logloss: 1.41918\n",
      "[861]\tvalid_0's multi_logloss: 1.41917\n",
      "[862]\tvalid_0's multi_logloss: 1.41912\n",
      "[863]\tvalid_0's multi_logloss: 1.41909\n",
      "[864]\tvalid_0's multi_logloss: 1.41911\n",
      "[865]\tvalid_0's multi_logloss: 1.41902\n",
      "[866]\tvalid_0's multi_logloss: 1.41899\n",
      "[867]\tvalid_0's multi_logloss: 1.41892\n",
      "[868]\tvalid_0's multi_logloss: 1.41882\n",
      "[869]\tvalid_0's multi_logloss: 1.41879\n",
      "[870]\tvalid_0's multi_logloss: 1.41873\n",
      "[871]\tvalid_0's multi_logloss: 1.41862\n",
      "[872]\tvalid_0's multi_logloss: 1.41857\n",
      "[873]\tvalid_0's multi_logloss: 1.41845\n",
      "[874]\tvalid_0's multi_logloss: 1.41842\n",
      "[875]\tvalid_0's multi_logloss: 1.41843\n",
      "[876]\tvalid_0's multi_logloss: 1.41845\n",
      "[877]\tvalid_0's multi_logloss: 1.41845\n",
      "[878]\tvalid_0's multi_logloss: 1.4184\n",
      "[879]\tvalid_0's multi_logloss: 1.4183\n",
      "[880]\tvalid_0's multi_logloss: 1.41831\n",
      "[881]\tvalid_0's multi_logloss: 1.41824\n",
      "[882]\tvalid_0's multi_logloss: 1.41821\n",
      "[883]\tvalid_0's multi_logloss: 1.41815\n",
      "[884]\tvalid_0's multi_logloss: 1.41811\n",
      "[885]\tvalid_0's multi_logloss: 1.41805\n",
      "[886]\tvalid_0's multi_logloss: 1.41803\n",
      "[887]\tvalid_0's multi_logloss: 1.41805\n",
      "[888]\tvalid_0's multi_logloss: 1.418\n",
      "[889]\tvalid_0's multi_logloss: 1.41795\n",
      "[890]\tvalid_0's multi_logloss: 1.41789\n",
      "[891]\tvalid_0's multi_logloss: 1.4178\n",
      "[892]\tvalid_0's multi_logloss: 1.41776\n",
      "[893]\tvalid_0's multi_logloss: 1.41773\n",
      "[894]\tvalid_0's multi_logloss: 1.41764\n",
      "[895]\tvalid_0's multi_logloss: 1.4176\n",
      "[896]\tvalid_0's multi_logloss: 1.41754\n",
      "[897]\tvalid_0's multi_logloss: 1.41753\n",
      "[898]\tvalid_0's multi_logloss: 1.41744\n",
      "[899]\tvalid_0's multi_logloss: 1.41741\n",
      "[900]\tvalid_0's multi_logloss: 1.41739\n",
      "[901]\tvalid_0's multi_logloss: 1.41736\n",
      "[902]\tvalid_0's multi_logloss: 1.4173\n",
      "[903]\tvalid_0's multi_logloss: 1.41724\n",
      "[904]\tvalid_0's multi_logloss: 1.41718\n",
      "[905]\tvalid_0's multi_logloss: 1.41714\n",
      "[906]\tvalid_0's multi_logloss: 1.41707\n",
      "[907]\tvalid_0's multi_logloss: 1.41705\n",
      "[908]\tvalid_0's multi_logloss: 1.41696\n",
      "[909]\tvalid_0's multi_logloss: 1.41686\n",
      "[910]\tvalid_0's multi_logloss: 1.4168\n",
      "[911]\tvalid_0's multi_logloss: 1.41674\n",
      "[912]\tvalid_0's multi_logloss: 1.41674\n",
      "[913]\tvalid_0's multi_logloss: 1.41668\n",
      "[914]\tvalid_0's multi_logloss: 1.41665\n",
      "[915]\tvalid_0's multi_logloss: 1.41661\n",
      "[916]\tvalid_0's multi_logloss: 1.41655\n",
      "[917]\tvalid_0's multi_logloss: 1.41653\n",
      "[918]\tvalid_0's multi_logloss: 1.41647\n",
      "[919]\tvalid_0's multi_logloss: 1.41648\n",
      "[920]\tvalid_0's multi_logloss: 1.41646\n",
      "[921]\tvalid_0's multi_logloss: 1.41645\n",
      "[922]\tvalid_0's multi_logloss: 1.41643\n",
      "[923]\tvalid_0's multi_logloss: 1.41638\n",
      "[924]\tvalid_0's multi_logloss: 1.41638\n",
      "[925]\tvalid_0's multi_logloss: 1.41631\n",
      "[926]\tvalid_0's multi_logloss: 1.41623\n",
      "[927]\tvalid_0's multi_logloss: 1.41615\n",
      "[928]\tvalid_0's multi_logloss: 1.41611\n",
      "[929]\tvalid_0's multi_logloss: 1.41613\n",
      "[930]\tvalid_0's multi_logloss: 1.41608\n",
      "[931]\tvalid_0's multi_logloss: 1.41607\n",
      "[932]\tvalid_0's multi_logloss: 1.41601\n",
      "[933]\tvalid_0's multi_logloss: 1.41597\n",
      "[934]\tvalid_0's multi_logloss: 1.41595\n",
      "[935]\tvalid_0's multi_logloss: 1.41586\n",
      "[936]\tvalid_0's multi_logloss: 1.41575\n",
      "[937]\tvalid_0's multi_logloss: 1.41573\n",
      "[938]\tvalid_0's multi_logloss: 1.41571\n",
      "[939]\tvalid_0's multi_logloss: 1.41571\n",
      "[940]\tvalid_0's multi_logloss: 1.4157\n",
      "[941]\tvalid_0's multi_logloss: 1.41568\n",
      "[942]\tvalid_0's multi_logloss: 1.41564\n",
      "[943]\tvalid_0's multi_logloss: 1.41558\n",
      "[944]\tvalid_0's multi_logloss: 1.41555\n",
      "[945]\tvalid_0's multi_logloss: 1.41554\n",
      "[946]\tvalid_0's multi_logloss: 1.41543\n",
      "[947]\tvalid_0's multi_logloss: 1.41538\n",
      "[948]\tvalid_0's multi_logloss: 1.4153\n",
      "[949]\tvalid_0's multi_logloss: 1.41525\n",
      "[950]\tvalid_0's multi_logloss: 1.41521\n",
      "[951]\tvalid_0's multi_logloss: 1.41517\n",
      "[952]\tvalid_0's multi_logloss: 1.41515\n",
      "[953]\tvalid_0's multi_logloss: 1.41513\n",
      "[954]\tvalid_0's multi_logloss: 1.41508\n",
      "[955]\tvalid_0's multi_logloss: 1.415\n",
      "[956]\tvalid_0's multi_logloss: 1.41495\n",
      "[957]\tvalid_0's multi_logloss: 1.41489\n",
      "[958]\tvalid_0's multi_logloss: 1.41482\n",
      "[959]\tvalid_0's multi_logloss: 1.41476\n",
      "[960]\tvalid_0's multi_logloss: 1.41475\n",
      "[961]\tvalid_0's multi_logloss: 1.41468\n",
      "[962]\tvalid_0's multi_logloss: 1.41461\n",
      "[963]\tvalid_0's multi_logloss: 1.41455\n",
      "[964]\tvalid_0's multi_logloss: 1.41454\n",
      "[965]\tvalid_0's multi_logloss: 1.41452\n",
      "[966]\tvalid_0's multi_logloss: 1.41454\n",
      "[967]\tvalid_0's multi_logloss: 1.41447\n",
      "[968]\tvalid_0's multi_logloss: 1.41439\n",
      "[969]\tvalid_0's multi_logloss: 1.41436\n",
      "[970]\tvalid_0's multi_logloss: 1.41427\n",
      "[971]\tvalid_0's multi_logloss: 1.41428\n",
      "[972]\tvalid_0's multi_logloss: 1.4143\n",
      "[973]\tvalid_0's multi_logloss: 1.41429\n",
      "[974]\tvalid_0's multi_logloss: 1.41431\n",
      "[975]\tvalid_0's multi_logloss: 1.41426\n",
      "[976]\tvalid_0's multi_logloss: 1.41418\n",
      "[977]\tvalid_0's multi_logloss: 1.41413\n",
      "[978]\tvalid_0's multi_logloss: 1.41417\n",
      "[979]\tvalid_0's multi_logloss: 1.41411\n",
      "[980]\tvalid_0's multi_logloss: 1.41408\n",
      "[981]\tvalid_0's multi_logloss: 1.41404\n",
      "[982]\tvalid_0's multi_logloss: 1.41397\n",
      "[983]\tvalid_0's multi_logloss: 1.41401\n",
      "[984]\tvalid_0's multi_logloss: 1.41395\n",
      "[985]\tvalid_0's multi_logloss: 1.41395\n",
      "[986]\tvalid_0's multi_logloss: 1.41391\n",
      "[987]\tvalid_0's multi_logloss: 1.41384\n",
      "[988]\tvalid_0's multi_logloss: 1.4138\n",
      "[989]\tvalid_0's multi_logloss: 1.41378\n",
      "[990]\tvalid_0's multi_logloss: 1.41371\n",
      "[991]\tvalid_0's multi_logloss: 1.41372\n",
      "[992]\tvalid_0's multi_logloss: 1.41368\n",
      "[993]\tvalid_0's multi_logloss: 1.41368\n",
      "[994]\tvalid_0's multi_logloss: 1.41364\n",
      "[995]\tvalid_0's multi_logloss: 1.41356\n",
      "[996]\tvalid_0's multi_logloss: 1.41354\n",
      "[997]\tvalid_0's multi_logloss: 1.41348\n",
      "[998]\tvalid_0's multi_logloss: 1.41339\n",
      "[999]\tvalid_0's multi_logloss: 1.41339\n",
      "[1000]\tvalid_0's multi_logloss: 1.41333\n",
      "[1001]\tvalid_0's multi_logloss: 1.4133\n",
      "[1002]\tvalid_0's multi_logloss: 1.41327\n",
      "[1003]\tvalid_0's multi_logloss: 1.41324\n",
      "[1004]\tvalid_0's multi_logloss: 1.41321\n",
      "[1005]\tvalid_0's multi_logloss: 1.41314\n",
      "[1006]\tvalid_0's multi_logloss: 1.41306\n",
      "[1007]\tvalid_0's multi_logloss: 1.41304\n",
      "[1008]\tvalid_0's multi_logloss: 1.41302\n",
      "[1009]\tvalid_0's multi_logloss: 1.41304\n",
      "[1010]\tvalid_0's multi_logloss: 1.41302\n",
      "[1011]\tvalid_0's multi_logloss: 1.41297\n",
      "[1012]\tvalid_0's multi_logloss: 1.41293\n",
      "[1013]\tvalid_0's multi_logloss: 1.41289\n",
      "[1014]\tvalid_0's multi_logloss: 1.4128\n",
      "[1015]\tvalid_0's multi_logloss: 1.41283\n",
      "[1016]\tvalid_0's multi_logloss: 1.41284\n",
      "[1017]\tvalid_0's multi_logloss: 1.41287\n",
      "[1018]\tvalid_0's multi_logloss: 1.41277\n",
      "[1019]\tvalid_0's multi_logloss: 1.41277\n",
      "[1020]\tvalid_0's multi_logloss: 1.41275\n",
      "[1021]\tvalid_0's multi_logloss: 1.41268\n",
      "[1022]\tvalid_0's multi_logloss: 1.41259\n",
      "[1023]\tvalid_0's multi_logloss: 1.41257\n",
      "[1024]\tvalid_0's multi_logloss: 1.41249\n",
      "[1025]\tvalid_0's multi_logloss: 1.41246\n",
      "[1026]\tvalid_0's multi_logloss: 1.41241\n",
      "[1027]\tvalid_0's multi_logloss: 1.41241\n",
      "[1028]\tvalid_0's multi_logloss: 1.41239\n",
      "[1029]\tvalid_0's multi_logloss: 1.4124\n",
      "[1030]\tvalid_0's multi_logloss: 1.4124\n",
      "[1031]\tvalid_0's multi_logloss: 1.41246\n",
      "[1032]\tvalid_0's multi_logloss: 1.41243\n",
      "[1033]\tvalid_0's multi_logloss: 1.41244\n",
      "[1034]\tvalid_0's multi_logloss: 1.41239\n",
      "[1035]\tvalid_0's multi_logloss: 1.41236\n",
      "[1036]\tvalid_0's multi_logloss: 1.41235\n",
      "[1037]\tvalid_0's multi_logloss: 1.41232\n",
      "[1038]\tvalid_0's multi_logloss: 1.41221\n",
      "[1039]\tvalid_0's multi_logloss: 1.41219\n",
      "[1040]\tvalid_0's multi_logloss: 1.41211\n",
      "[1041]\tvalid_0's multi_logloss: 1.41208\n",
      "[1042]\tvalid_0's multi_logloss: 1.41208\n",
      "[1043]\tvalid_0's multi_logloss: 1.41205\n",
      "[1044]\tvalid_0's multi_logloss: 1.41196\n",
      "[1045]\tvalid_0's multi_logloss: 1.41194\n",
      "[1046]\tvalid_0's multi_logloss: 1.4119\n",
      "[1047]\tvalid_0's multi_logloss: 1.41194\n",
      "[1048]\tvalid_0's multi_logloss: 1.41194\n",
      "[1049]\tvalid_0's multi_logloss: 1.41187\n",
      "[1050]\tvalid_0's multi_logloss: 1.4119\n",
      "[1051]\tvalid_0's multi_logloss: 1.4119\n",
      "[1052]\tvalid_0's multi_logloss: 1.41187\n",
      "[1053]\tvalid_0's multi_logloss: 1.41182\n",
      "[1054]\tvalid_0's multi_logloss: 1.41173\n",
      "[1055]\tvalid_0's multi_logloss: 1.41169\n",
      "[1056]\tvalid_0's multi_logloss: 1.41167\n",
      "[1057]\tvalid_0's multi_logloss: 1.41165\n",
      "[1058]\tvalid_0's multi_logloss: 1.41167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1059]\tvalid_0's multi_logloss: 1.41168\n",
      "[1060]\tvalid_0's multi_logloss: 1.4116\n",
      "[1061]\tvalid_0's multi_logloss: 1.41161\n",
      "[1062]\tvalid_0's multi_logloss: 1.41156\n",
      "[1063]\tvalid_0's multi_logloss: 1.41152\n",
      "[1064]\tvalid_0's multi_logloss: 1.41151\n",
      "[1065]\tvalid_0's multi_logloss: 1.41143\n",
      "[1066]\tvalid_0's multi_logloss: 1.4114\n",
      "[1067]\tvalid_0's multi_logloss: 1.4114\n",
      "[1068]\tvalid_0's multi_logloss: 1.41141\n",
      "[1069]\tvalid_0's multi_logloss: 1.41136\n",
      "[1070]\tvalid_0's multi_logloss: 1.41136\n",
      "[1071]\tvalid_0's multi_logloss: 1.41134\n",
      "[1072]\tvalid_0's multi_logloss: 1.41134\n",
      "[1073]\tvalid_0's multi_logloss: 1.41135\n",
      "[1074]\tvalid_0's multi_logloss: 1.41132\n",
      "[1075]\tvalid_0's multi_logloss: 1.4113\n",
      "[1076]\tvalid_0's multi_logloss: 1.41126\n",
      "[1077]\tvalid_0's multi_logloss: 1.41119\n",
      "[1078]\tvalid_0's multi_logloss: 1.41111\n",
      "[1079]\tvalid_0's multi_logloss: 1.41117\n",
      "[1080]\tvalid_0's multi_logloss: 1.41115\n",
      "[1081]\tvalid_0's multi_logloss: 1.41111\n",
      "[1082]\tvalid_0's multi_logloss: 1.41107\n",
      "[1083]\tvalid_0's multi_logloss: 1.41105\n",
      "[1084]\tvalid_0's multi_logloss: 1.41102\n",
      "[1085]\tvalid_0's multi_logloss: 1.411\n",
      "[1086]\tvalid_0's multi_logloss: 1.41105\n",
      "[1087]\tvalid_0's multi_logloss: 1.41097\n",
      "[1088]\tvalid_0's multi_logloss: 1.41097\n",
      "[1089]\tvalid_0's multi_logloss: 1.41103\n",
      "[1090]\tvalid_0's multi_logloss: 1.41102\n",
      "[1091]\tvalid_0's multi_logloss: 1.41104\n",
      "[1092]\tvalid_0's multi_logloss: 1.41102\n",
      "[1093]\tvalid_0's multi_logloss: 1.41093\n",
      "[1094]\tvalid_0's multi_logloss: 1.4109\n",
      "[1095]\tvalid_0's multi_logloss: 1.41092\n",
      "[1096]\tvalid_0's multi_logloss: 1.41089\n",
      "[1097]\tvalid_0's multi_logloss: 1.41086\n",
      "[1098]\tvalid_0's multi_logloss: 1.41081\n",
      "[1099]\tvalid_0's multi_logloss: 1.41076\n",
      "[1100]\tvalid_0's multi_logloss: 1.41075\n",
      "[1101]\tvalid_0's multi_logloss: 1.41077\n",
      "[1102]\tvalid_0's multi_logloss: 1.41076\n",
      "[1103]\tvalid_0's multi_logloss: 1.41073\n",
      "[1104]\tvalid_0's multi_logloss: 1.41071\n",
      "[1105]\tvalid_0's multi_logloss: 1.41067\n",
      "[1106]\tvalid_0's multi_logloss: 1.41063\n",
      "[1107]\tvalid_0's multi_logloss: 1.41057\n",
      "[1108]\tvalid_0's multi_logloss: 1.41053\n",
      "[1109]\tvalid_0's multi_logloss: 1.41048\n",
      "[1110]\tvalid_0's multi_logloss: 1.41042\n",
      "[1111]\tvalid_0's multi_logloss: 1.41041\n",
      "[1112]\tvalid_0's multi_logloss: 1.41042\n",
      "[1113]\tvalid_0's multi_logloss: 1.41039\n",
      "[1114]\tvalid_0's multi_logloss: 1.41036\n",
      "[1115]\tvalid_0's multi_logloss: 1.41037\n",
      "[1116]\tvalid_0's multi_logloss: 1.41037\n",
      "[1117]\tvalid_0's multi_logloss: 1.41034\n",
      "[1118]\tvalid_0's multi_logloss: 1.41029\n",
      "[1119]\tvalid_0's multi_logloss: 1.41024\n",
      "[1120]\tvalid_0's multi_logloss: 1.41028\n",
      "[1121]\tvalid_0's multi_logloss: 1.41021\n",
      "[1122]\tvalid_0's multi_logloss: 1.41014\n",
      "[1123]\tvalid_0's multi_logloss: 1.41014\n",
      "[1124]\tvalid_0's multi_logloss: 1.41013\n",
      "[1125]\tvalid_0's multi_logloss: 1.41005\n",
      "[1126]\tvalid_0's multi_logloss: 1.41003\n",
      "[1127]\tvalid_0's multi_logloss: 1.41001\n",
      "[1128]\tvalid_0's multi_logloss: 1.40997\n",
      "[1129]\tvalid_0's multi_logloss: 1.40995\n",
      "[1130]\tvalid_0's multi_logloss: 1.40991\n",
      "[1131]\tvalid_0's multi_logloss: 1.40982\n",
      "[1132]\tvalid_0's multi_logloss: 1.40979\n",
      "[1133]\tvalid_0's multi_logloss: 1.40978\n",
      "[1134]\tvalid_0's multi_logloss: 1.40978\n",
      "[1135]\tvalid_0's multi_logloss: 1.40973\n",
      "[1136]\tvalid_0's multi_logloss: 1.40975\n",
      "[1137]\tvalid_0's multi_logloss: 1.40975\n",
      "[1138]\tvalid_0's multi_logloss: 1.40967\n",
      "[1139]\tvalid_0's multi_logloss: 1.40964\n",
      "[1140]\tvalid_0's multi_logloss: 1.40964\n",
      "[1141]\tvalid_0's multi_logloss: 1.40955\n",
      "[1142]\tvalid_0's multi_logloss: 1.40952\n",
      "[1143]\tvalid_0's multi_logloss: 1.40946\n",
      "[1144]\tvalid_0's multi_logloss: 1.40946\n",
      "[1145]\tvalid_0's multi_logloss: 1.40946\n",
      "[1146]\tvalid_0's multi_logloss: 1.40946\n",
      "[1147]\tvalid_0's multi_logloss: 1.40948\n",
      "[1148]\tvalid_0's multi_logloss: 1.40943\n",
      "[1149]\tvalid_0's multi_logloss: 1.40938\n",
      "[1150]\tvalid_0's multi_logloss: 1.40941\n",
      "[1151]\tvalid_0's multi_logloss: 1.40934\n",
      "[1152]\tvalid_0's multi_logloss: 1.40934\n",
      "[1153]\tvalid_0's multi_logloss: 1.40932\n",
      "[1154]\tvalid_0's multi_logloss: 1.40928\n",
      "[1155]\tvalid_0's multi_logloss: 1.40929\n",
      "[1156]\tvalid_0's multi_logloss: 1.40929\n",
      "[1157]\tvalid_0's multi_logloss: 1.40928\n",
      "[1158]\tvalid_0's multi_logloss: 1.40926\n",
      "[1159]\tvalid_0's multi_logloss: 1.40922\n",
      "[1160]\tvalid_0's multi_logloss: 1.40919\n",
      "[1161]\tvalid_0's multi_logloss: 1.40909\n",
      "[1162]\tvalid_0's multi_logloss: 1.40912\n",
      "[1163]\tvalid_0's multi_logloss: 1.40909\n",
      "[1164]\tvalid_0's multi_logloss: 1.40902\n",
      "[1165]\tvalid_0's multi_logloss: 1.40897\n",
      "[1166]\tvalid_0's multi_logloss: 1.40893\n",
      "[1167]\tvalid_0's multi_logloss: 1.4089\n",
      "[1168]\tvalid_0's multi_logloss: 1.40889\n",
      "[1169]\tvalid_0's multi_logloss: 1.40889\n",
      "[1170]\tvalid_0's multi_logloss: 1.40882\n",
      "[1171]\tvalid_0's multi_logloss: 1.40878\n",
      "[1172]\tvalid_0's multi_logloss: 1.40874\n",
      "[1173]\tvalid_0's multi_logloss: 1.40866\n",
      "[1174]\tvalid_0's multi_logloss: 1.40867\n",
      "[1175]\tvalid_0's multi_logloss: 1.40869\n",
      "[1176]\tvalid_0's multi_logloss: 1.40866\n",
      "[1177]\tvalid_0's multi_logloss: 1.40866\n",
      "[1178]\tvalid_0's multi_logloss: 1.4086\n",
      "[1179]\tvalid_0's multi_logloss: 1.40853\n",
      "[1180]\tvalid_0's multi_logloss: 1.40855\n",
      "[1181]\tvalid_0's multi_logloss: 1.40852\n",
      "[1182]\tvalid_0's multi_logloss: 1.40853\n",
      "[1183]\tvalid_0's multi_logloss: 1.40852\n",
      "[1184]\tvalid_0's multi_logloss: 1.40854\n",
      "[1185]\tvalid_0's multi_logloss: 1.40846\n",
      "[1186]\tvalid_0's multi_logloss: 1.40844\n",
      "[1187]\tvalid_0's multi_logloss: 1.40833\n",
      "[1188]\tvalid_0's multi_logloss: 1.40836\n",
      "[1189]\tvalid_0's multi_logloss: 1.4083\n",
      "[1190]\tvalid_0's multi_logloss: 1.40831\n",
      "[1191]\tvalid_0's multi_logloss: 1.40828\n",
      "[1192]\tvalid_0's multi_logloss: 1.40828\n",
      "[1193]\tvalid_0's multi_logloss: 1.4083\n",
      "[1194]\tvalid_0's multi_logloss: 1.40826\n",
      "[1195]\tvalid_0's multi_logloss: 1.40826\n",
      "[1196]\tvalid_0's multi_logloss: 1.40822\n",
      "[1197]\tvalid_0's multi_logloss: 1.40823\n",
      "[1198]\tvalid_0's multi_logloss: 1.40824\n",
      "[1199]\tvalid_0's multi_logloss: 1.40817\n",
      "[1200]\tvalid_0's multi_logloss: 1.40811\n",
      "[1201]\tvalid_0's multi_logloss: 1.4081\n",
      "[1202]\tvalid_0's multi_logloss: 1.40804\n",
      "[1203]\tvalid_0's multi_logloss: 1.40804\n",
      "[1204]\tvalid_0's multi_logloss: 1.40803\n",
      "[1205]\tvalid_0's multi_logloss: 1.40805\n",
      "[1206]\tvalid_0's multi_logloss: 1.40805\n",
      "[1207]\tvalid_0's multi_logloss: 1.40802\n",
      "[1208]\tvalid_0's multi_logloss: 1.40795\n",
      "[1209]\tvalid_0's multi_logloss: 1.40793\n",
      "[1210]\tvalid_0's multi_logloss: 1.40797\n",
      "[1211]\tvalid_0's multi_logloss: 1.40792\n",
      "[1212]\tvalid_0's multi_logloss: 1.4079\n",
      "[1213]\tvalid_0's multi_logloss: 1.40794\n",
      "[1214]\tvalid_0's multi_logloss: 1.40793\n",
      "[1215]\tvalid_0's multi_logloss: 1.4079\n",
      "[1216]\tvalid_0's multi_logloss: 1.40785\n",
      "[1217]\tvalid_0's multi_logloss: 1.40781\n",
      "[1218]\tvalid_0's multi_logloss: 1.40781\n",
      "[1219]\tvalid_0's multi_logloss: 1.40781\n",
      "[1220]\tvalid_0's multi_logloss: 1.40773\n",
      "[1221]\tvalid_0's multi_logloss: 1.40769\n",
      "[1222]\tvalid_0's multi_logloss: 1.4077\n",
      "[1223]\tvalid_0's multi_logloss: 1.40767\n",
      "[1224]\tvalid_0's multi_logloss: 1.40772\n",
      "[1225]\tvalid_0's multi_logloss: 1.40773\n",
      "[1226]\tvalid_0's multi_logloss: 1.40772\n",
      "[1227]\tvalid_0's multi_logloss: 1.40771\n",
      "[1228]\tvalid_0's multi_logloss: 1.40772\n",
      "[1229]\tvalid_0's multi_logloss: 1.40772\n",
      "[1230]\tvalid_0's multi_logloss: 1.40772\n",
      "[1231]\tvalid_0's multi_logloss: 1.40775\n",
      "[1232]\tvalid_0's multi_logloss: 1.40772\n",
      "[1233]\tvalid_0's multi_logloss: 1.40771\n",
      "[1234]\tvalid_0's multi_logloss: 1.40763\n",
      "[1235]\tvalid_0's multi_logloss: 1.40759\n",
      "[1236]\tvalid_0's multi_logloss: 1.40752\n",
      "[1237]\tvalid_0's multi_logloss: 1.40749\n",
      "[1238]\tvalid_0's multi_logloss: 1.40752\n",
      "[1239]\tvalid_0's multi_logloss: 1.40753\n",
      "[1240]\tvalid_0's multi_logloss: 1.40752\n",
      "[1241]\tvalid_0's multi_logloss: 1.40755\n",
      "[1242]\tvalid_0's multi_logloss: 1.4076\n",
      "[1243]\tvalid_0's multi_logloss: 1.40756\n",
      "[1244]\tvalid_0's multi_logloss: 1.40752\n",
      "[1245]\tvalid_0's multi_logloss: 1.40745\n",
      "[1246]\tvalid_0's multi_logloss: 1.40747\n",
      "[1247]\tvalid_0's multi_logloss: 1.40744\n",
      "[1248]\tvalid_0's multi_logloss: 1.40744\n",
      "[1249]\tvalid_0's multi_logloss: 1.40739\n",
      "[1250]\tvalid_0's multi_logloss: 1.4074\n",
      "[1251]\tvalid_0's multi_logloss: 1.40739\n",
      "[1252]\tvalid_0's multi_logloss: 1.40734\n",
      "[1253]\tvalid_0's multi_logloss: 1.40737\n",
      "[1254]\tvalid_0's multi_logloss: 1.40742\n",
      "[1255]\tvalid_0's multi_logloss: 1.40739\n",
      "[1256]\tvalid_0's multi_logloss: 1.40738\n",
      "[1257]\tvalid_0's multi_logloss: 1.40737\n",
      "[1258]\tvalid_0's multi_logloss: 1.40736\n",
      "[1259]\tvalid_0's multi_logloss: 1.40735\n",
      "[1260]\tvalid_0's multi_logloss: 1.40728\n",
      "[1261]\tvalid_0's multi_logloss: 1.40728\n",
      "[1262]\tvalid_0's multi_logloss: 1.40724\n",
      "[1263]\tvalid_0's multi_logloss: 1.40722\n",
      "[1264]\tvalid_0's multi_logloss: 1.40719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1265]\tvalid_0's multi_logloss: 1.40722\n",
      "[1266]\tvalid_0's multi_logloss: 1.40724\n",
      "[1267]\tvalid_0's multi_logloss: 1.40719\n",
      "[1268]\tvalid_0's multi_logloss: 1.40718\n",
      "[1269]\tvalid_0's multi_logloss: 1.40719\n",
      "[1270]\tvalid_0's multi_logloss: 1.40717\n",
      "[1271]\tvalid_0's multi_logloss: 1.4072\n",
      "[1272]\tvalid_0's multi_logloss: 1.40714\n",
      "[1273]\tvalid_0's multi_logloss: 1.40718\n",
      "[1274]\tvalid_0's multi_logloss: 1.4071\n",
      "[1275]\tvalid_0's multi_logloss: 1.4071\n",
      "[1276]\tvalid_0's multi_logloss: 1.40707\n",
      "[1277]\tvalid_0's multi_logloss: 1.40707\n",
      "[1278]\tvalid_0's multi_logloss: 1.40707\n",
      "[1279]\tvalid_0's multi_logloss: 1.40711\n",
      "[1280]\tvalid_0's multi_logloss: 1.40709\n",
      "[1281]\tvalid_0's multi_logloss: 1.40707\n",
      "[1282]\tvalid_0's multi_logloss: 1.40704\n",
      "[1283]\tvalid_0's multi_logloss: 1.40699\n",
      "[1284]\tvalid_0's multi_logloss: 1.407\n",
      "[1285]\tvalid_0's multi_logloss: 1.40691\n",
      "[1286]\tvalid_0's multi_logloss: 1.40687\n",
      "[1287]\tvalid_0's multi_logloss: 1.40684\n",
      "[1288]\tvalid_0's multi_logloss: 1.40687\n",
      "[1289]\tvalid_0's multi_logloss: 1.40691\n",
      "[1290]\tvalid_0's multi_logloss: 1.40689\n",
      "[1291]\tvalid_0's multi_logloss: 1.40689\n",
      "[1292]\tvalid_0's multi_logloss: 1.40683\n",
      "[1293]\tvalid_0's multi_logloss: 1.40685\n",
      "[1294]\tvalid_0's multi_logloss: 1.4068\n",
      "[1295]\tvalid_0's multi_logloss: 1.40677\n",
      "[1296]\tvalid_0's multi_logloss: 1.4068\n",
      "[1297]\tvalid_0's multi_logloss: 1.40678\n",
      "[1298]\tvalid_0's multi_logloss: 1.40673\n",
      "[1299]\tvalid_0's multi_logloss: 1.40674\n",
      "[1300]\tvalid_0's multi_logloss: 1.40674\n",
      "[1301]\tvalid_0's multi_logloss: 1.40669\n",
      "[1302]\tvalid_0's multi_logloss: 1.40666\n",
      "[1303]\tvalid_0's multi_logloss: 1.40663\n",
      "[1304]\tvalid_0's multi_logloss: 1.40656\n",
      "[1305]\tvalid_0's multi_logloss: 1.40653\n",
      "[1306]\tvalid_0's multi_logloss: 1.4065\n",
      "[1307]\tvalid_0's multi_logloss: 1.40649\n",
      "[1308]\tvalid_0's multi_logloss: 1.40652\n",
      "[1309]\tvalid_0's multi_logloss: 1.40648\n",
      "[1310]\tvalid_0's multi_logloss: 1.4065\n",
      "[1311]\tvalid_0's multi_logloss: 1.40646\n",
      "[1312]\tvalid_0's multi_logloss: 1.40649\n",
      "[1313]\tvalid_0's multi_logloss: 1.40649\n",
      "[1314]\tvalid_0's multi_logloss: 1.40647\n",
      "[1315]\tvalid_0's multi_logloss: 1.40654\n",
      "[1316]\tvalid_0's multi_logloss: 1.40652\n",
      "[1317]\tvalid_0's multi_logloss: 1.4065\n",
      "[1318]\tvalid_0's multi_logloss: 1.40648\n",
      "[1319]\tvalid_0's multi_logloss: 1.40652\n",
      "[1320]\tvalid_0's multi_logloss: 1.40655\n",
      "[1321]\tvalid_0's multi_logloss: 1.40654\n",
      "[1322]\tvalid_0's multi_logloss: 1.40651\n",
      "[1323]\tvalid_0's multi_logloss: 1.40646\n",
      "[1324]\tvalid_0's multi_logloss: 1.4065\n",
      "[1325]\tvalid_0's multi_logloss: 1.40652\n",
      "[1326]\tvalid_0's multi_logloss: 1.40652\n",
      "[1327]\tvalid_0's multi_logloss: 1.4065\n",
      "[1328]\tvalid_0's multi_logloss: 1.40648\n",
      "[1329]\tvalid_0's multi_logloss: 1.40652\n",
      "[1330]\tvalid_0's multi_logloss: 1.40658\n",
      "[1331]\tvalid_0's multi_logloss: 1.40658\n",
      "[1332]\tvalid_0's multi_logloss: 1.40663\n",
      "[1333]\tvalid_0's multi_logloss: 1.40661\n",
      "[1334]\tvalid_0's multi_logloss: 1.40663\n",
      "[1335]\tvalid_0's multi_logloss: 1.40659\n",
      "[1336]\tvalid_0's multi_logloss: 1.4066\n",
      "[1337]\tvalid_0's multi_logloss: 1.40656\n",
      "[1338]\tvalid_0's multi_logloss: 1.40652\n",
      "[1339]\tvalid_0's multi_logloss: 1.40657\n",
      "[1340]\tvalid_0's multi_logloss: 1.40653\n",
      "[1341]\tvalid_0's multi_logloss: 1.40652\n",
      "[1342]\tvalid_0's multi_logloss: 1.40651\n",
      "[1343]\tvalid_0's multi_logloss: 1.40647\n",
      "[1344]\tvalid_0's multi_logloss: 1.40648\n",
      "[1345]\tvalid_0's multi_logloss: 1.40648\n",
      "[1346]\tvalid_0's multi_logloss: 1.40639\n",
      "[1347]\tvalid_0's multi_logloss: 1.40638\n",
      "[1348]\tvalid_0's multi_logloss: 1.40638\n",
      "[1349]\tvalid_0's multi_logloss: 1.40631\n",
      "[1350]\tvalid_0's multi_logloss: 1.4063\n",
      "[1351]\tvalid_0's multi_logloss: 1.4063\n",
      "[1352]\tvalid_0's multi_logloss: 1.40631\n",
      "[1353]\tvalid_0's multi_logloss: 1.40631\n",
      "[1354]\tvalid_0's multi_logloss: 1.40629\n",
      "[1355]\tvalid_0's multi_logloss: 1.40626\n",
      "[1356]\tvalid_0's multi_logloss: 1.40629\n",
      "[1357]\tvalid_0's multi_logloss: 1.40628\n",
      "[1358]\tvalid_0's multi_logloss: 1.40623\n",
      "[1359]\tvalid_0's multi_logloss: 1.40619\n",
      "[1360]\tvalid_0's multi_logloss: 1.40621\n",
      "[1361]\tvalid_0's multi_logloss: 1.40621\n",
      "[1362]\tvalid_0's multi_logloss: 1.40624\n",
      "[1363]\tvalid_0's multi_logloss: 1.40622\n",
      "[1364]\tvalid_0's multi_logloss: 1.40621\n",
      "[1365]\tvalid_0's multi_logloss: 1.40618\n",
      "[1366]\tvalid_0's multi_logloss: 1.40619\n",
      "[1367]\tvalid_0's multi_logloss: 1.40614\n",
      "[1368]\tvalid_0's multi_logloss: 1.40614\n",
      "[1369]\tvalid_0's multi_logloss: 1.40613\n",
      "[1370]\tvalid_0's multi_logloss: 1.40611\n",
      "[1371]\tvalid_0's multi_logloss: 1.40613\n",
      "[1372]\tvalid_0's multi_logloss: 1.40613\n",
      "[1373]\tvalid_0's multi_logloss: 1.40609\n",
      "[1374]\tvalid_0's multi_logloss: 1.40605\n",
      "[1375]\tvalid_0's multi_logloss: 1.406\n",
      "[1376]\tvalid_0's multi_logloss: 1.40595\n",
      "[1377]\tvalid_0's multi_logloss: 1.40596\n",
      "[1378]\tvalid_0's multi_logloss: 1.40595\n",
      "[1379]\tvalid_0's multi_logloss: 1.40601\n",
      "[1380]\tvalid_0's multi_logloss: 1.406\n",
      "[1381]\tvalid_0's multi_logloss: 1.406\n",
      "[1382]\tvalid_0's multi_logloss: 1.40599\n",
      "[1383]\tvalid_0's multi_logloss: 1.40597\n",
      "[1384]\tvalid_0's multi_logloss: 1.40596\n",
      "[1385]\tvalid_0's multi_logloss: 1.40593\n",
      "[1386]\tvalid_0's multi_logloss: 1.4059\n",
      "[1387]\tvalid_0's multi_logloss: 1.40588\n",
      "[1388]\tvalid_0's multi_logloss: 1.40587\n",
      "[1389]\tvalid_0's multi_logloss: 1.40585\n",
      "[1390]\tvalid_0's multi_logloss: 1.40587\n",
      "[1391]\tvalid_0's multi_logloss: 1.40584\n",
      "[1392]\tvalid_0's multi_logloss: 1.40587\n",
      "[1393]\tvalid_0's multi_logloss: 1.40589\n",
      "[1394]\tvalid_0's multi_logloss: 1.40586\n",
      "[1395]\tvalid_0's multi_logloss: 1.40588\n",
      "[1396]\tvalid_0's multi_logloss: 1.40579\n",
      "[1397]\tvalid_0's multi_logloss: 1.40578\n",
      "[1398]\tvalid_0's multi_logloss: 1.40576\n",
      "[1399]\tvalid_0's multi_logloss: 1.40577\n",
      "[1400]\tvalid_0's multi_logloss: 1.40579\n",
      "[1401]\tvalid_0's multi_logloss: 1.40578\n",
      "[1402]\tvalid_0's multi_logloss: 1.40578\n",
      "[1403]\tvalid_0's multi_logloss: 1.40577\n",
      "[1404]\tvalid_0's multi_logloss: 1.40577\n",
      "[1405]\tvalid_0's multi_logloss: 1.40577\n",
      "[1406]\tvalid_0's multi_logloss: 1.40578\n",
      "[1407]\tvalid_0's multi_logloss: 1.40577\n",
      "[1408]\tvalid_0's multi_logloss: 1.40576\n",
      "[1409]\tvalid_0's multi_logloss: 1.40572\n",
      "[1410]\tvalid_0's multi_logloss: 1.40572\n",
      "[1411]\tvalid_0's multi_logloss: 1.40573\n",
      "[1412]\tvalid_0's multi_logloss: 1.40567\n",
      "[1413]\tvalid_0's multi_logloss: 1.40568\n",
      "[1414]\tvalid_0's multi_logloss: 1.40562\n",
      "[1415]\tvalid_0's multi_logloss: 1.40565\n",
      "[1416]\tvalid_0's multi_logloss: 1.40565\n",
      "[1417]\tvalid_0's multi_logloss: 1.4057\n",
      "[1418]\tvalid_0's multi_logloss: 1.40569\n",
      "[1419]\tvalid_0's multi_logloss: 1.40563\n",
      "[1420]\tvalid_0's multi_logloss: 1.40565\n",
      "[1421]\tvalid_0's multi_logloss: 1.40562\n",
      "[1422]\tvalid_0's multi_logloss: 1.40557\n",
      "[1423]\tvalid_0's multi_logloss: 1.40559\n",
      "[1424]\tvalid_0's multi_logloss: 1.40559\n",
      "[1425]\tvalid_0's multi_logloss: 1.40556\n",
      "[1426]\tvalid_0's multi_logloss: 1.40554\n",
      "[1427]\tvalid_0's multi_logloss: 1.40553\n",
      "[1428]\tvalid_0's multi_logloss: 1.40552\n",
      "[1429]\tvalid_0's multi_logloss: 1.40554\n",
      "[1430]\tvalid_0's multi_logloss: 1.40554\n",
      "[1431]\tvalid_0's multi_logloss: 1.40554\n",
      "[1432]\tvalid_0's multi_logloss: 1.40557\n",
      "[1433]\tvalid_0's multi_logloss: 1.40553\n",
      "[1434]\tvalid_0's multi_logloss: 1.40555\n",
      "[1435]\tvalid_0's multi_logloss: 1.40555\n",
      "[1436]\tvalid_0's multi_logloss: 1.40553\n",
      "[1437]\tvalid_0's multi_logloss: 1.40555\n",
      "[1438]\tvalid_0's multi_logloss: 1.40552\n",
      "[1439]\tvalid_0's multi_logloss: 1.40552\n",
      "[1440]\tvalid_0's multi_logloss: 1.40551\n",
      "[1441]\tvalid_0's multi_logloss: 1.40549\n",
      "[1442]\tvalid_0's multi_logloss: 1.40551\n",
      "[1443]\tvalid_0's multi_logloss: 1.40547\n",
      "[1444]\tvalid_0's multi_logloss: 1.40544\n",
      "[1445]\tvalid_0's multi_logloss: 1.40541\n",
      "[1446]\tvalid_0's multi_logloss: 1.40544\n",
      "[1447]\tvalid_0's multi_logloss: 1.40542\n",
      "[1448]\tvalid_0's multi_logloss: 1.4055\n",
      "[1449]\tvalid_0's multi_logloss: 1.4055\n",
      "[1450]\tvalid_0's multi_logloss: 1.4055\n",
      "[1451]\tvalid_0's multi_logloss: 1.4055\n",
      "[1452]\tvalid_0's multi_logloss: 1.40549\n",
      "[1453]\tvalid_0's multi_logloss: 1.4055\n",
      "[1454]\tvalid_0's multi_logloss: 1.40549\n",
      "[1455]\tvalid_0's multi_logloss: 1.40551\n",
      "[1456]\tvalid_0's multi_logloss: 1.40549\n",
      "[1457]\tvalid_0's multi_logloss: 1.40552\n",
      "[1458]\tvalid_0's multi_logloss: 1.40548\n",
      "[1459]\tvalid_0's multi_logloss: 1.40547\n",
      "[1460]\tvalid_0's multi_logloss: 1.40548\n",
      "[1461]\tvalid_0's multi_logloss: 1.40547\n",
      "[1462]\tvalid_0's multi_logloss: 1.40544\n",
      "[1463]\tvalid_0's multi_logloss: 1.40542\n",
      "[1464]\tvalid_0's multi_logloss: 1.40536\n",
      "[1465]\tvalid_0's multi_logloss: 1.40535\n",
      "[1466]\tvalid_0's multi_logloss: 1.40536\n",
      "[1467]\tvalid_0's multi_logloss: 1.40536\n",
      "[1468]\tvalid_0's multi_logloss: 1.40537\n",
      "[1469]\tvalid_0's multi_logloss: 1.40532\n",
      "[1470]\tvalid_0's multi_logloss: 1.40534\n",
      "[1471]\tvalid_0's multi_logloss: 1.40531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1472]\tvalid_0's multi_logloss: 1.40526\n",
      "[1473]\tvalid_0's multi_logloss: 1.4053\n",
      "[1474]\tvalid_0's multi_logloss: 1.4053\n",
      "[1475]\tvalid_0's multi_logloss: 1.4053\n",
      "[1476]\tvalid_0's multi_logloss: 1.4053\n",
      "[1477]\tvalid_0's multi_logloss: 1.4053\n",
      "[1478]\tvalid_0's multi_logloss: 1.40532\n",
      "[1479]\tvalid_0's multi_logloss: 1.40529\n",
      "[1480]\tvalid_0's multi_logloss: 1.40528\n",
      "[1481]\tvalid_0's multi_logloss: 1.40528\n",
      "[1482]\tvalid_0's multi_logloss: 1.40527\n",
      "[1483]\tvalid_0's multi_logloss: 1.40526\n",
      "[1484]\tvalid_0's multi_logloss: 1.40527\n",
      "[1485]\tvalid_0's multi_logloss: 1.40524\n",
      "[1486]\tvalid_0's multi_logloss: 1.40529\n",
      "[1487]\tvalid_0's multi_logloss: 1.40525\n",
      "[1488]\tvalid_0's multi_logloss: 1.40524\n",
      "[1489]\tvalid_0's multi_logloss: 1.40522\n",
      "[1490]\tvalid_0's multi_logloss: 1.40521\n",
      "[1491]\tvalid_0's multi_logloss: 1.40516\n",
      "[1492]\tvalid_0's multi_logloss: 1.40516\n",
      "[1493]\tvalid_0's multi_logloss: 1.40516\n",
      "[1494]\tvalid_0's multi_logloss: 1.40516\n",
      "[1495]\tvalid_0's multi_logloss: 1.40519\n",
      "[1496]\tvalid_0's multi_logloss: 1.40523\n",
      "[1497]\tvalid_0's multi_logloss: 1.4052\n",
      "[1498]\tvalid_0's multi_logloss: 1.40516\n",
      "[1499]\tvalid_0's multi_logloss: 1.40514\n",
      "[1500]\tvalid_0's multi_logloss: 1.40512\n",
      "[1501]\tvalid_0's multi_logloss: 1.40512\n",
      "[1502]\tvalid_0's multi_logloss: 1.4051\n",
      "[1503]\tvalid_0's multi_logloss: 1.40509\n",
      "[1504]\tvalid_0's multi_logloss: 1.40507\n",
      "[1505]\tvalid_0's multi_logloss: 1.40507\n",
      "[1506]\tvalid_0's multi_logloss: 1.4051\n",
      "[1507]\tvalid_0's multi_logloss: 1.40512\n",
      "[1508]\tvalid_0's multi_logloss: 1.40511\n",
      "[1509]\tvalid_0's multi_logloss: 1.40509\n",
      "[1510]\tvalid_0's multi_logloss: 1.40509\n",
      "[1511]\tvalid_0's multi_logloss: 1.40511\n",
      "[1512]\tvalid_0's multi_logloss: 1.40516\n",
      "[1513]\tvalid_0's multi_logloss: 1.40513\n",
      "[1514]\tvalid_0's multi_logloss: 1.40514\n",
      "[1515]\tvalid_0's multi_logloss: 1.40514\n",
      "[1516]\tvalid_0's multi_logloss: 1.40516\n",
      "[1517]\tvalid_0's multi_logloss: 1.40514\n",
      "[1518]\tvalid_0's multi_logloss: 1.40514\n",
      "[1519]\tvalid_0's multi_logloss: 1.40518\n",
      "[1520]\tvalid_0's multi_logloss: 1.4052\n",
      "[1521]\tvalid_0's multi_logloss: 1.40515\n",
      "[1522]\tvalid_0's multi_logloss: 1.40513\n",
      "[1523]\tvalid_0's multi_logloss: 1.4051\n",
      "[1524]\tvalid_0's multi_logloss: 1.40508\n",
      "[1525]\tvalid_0's multi_logloss: 1.40511\n",
      "[1526]\tvalid_0's multi_logloss: 1.40512\n",
      "[1527]\tvalid_0's multi_logloss: 1.40509\n",
      "[1528]\tvalid_0's multi_logloss: 1.40511\n",
      "[1529]\tvalid_0's multi_logloss: 1.40511\n",
      "[1530]\tvalid_0's multi_logloss: 1.40508\n",
      "[1531]\tvalid_0's multi_logloss: 1.4051\n",
      "[1532]\tvalid_0's multi_logloss: 1.40509\n",
      "[1533]\tvalid_0's multi_logloss: 1.40505\n",
      "[1534]\tvalid_0's multi_logloss: 1.40505\n",
      "[1535]\tvalid_0's multi_logloss: 1.40505\n",
      "[1536]\tvalid_0's multi_logloss: 1.40508\n",
      "[1537]\tvalid_0's multi_logloss: 1.40508\n",
      "[1538]\tvalid_0's multi_logloss: 1.40512\n",
      "[1539]\tvalid_0's multi_logloss: 1.40511\n",
      "[1540]\tvalid_0's multi_logloss: 1.40512\n",
      "[1541]\tvalid_0's multi_logloss: 1.40508\n",
      "[1542]\tvalid_0's multi_logloss: 1.40509\n",
      "[1543]\tvalid_0's multi_logloss: 1.40504\n",
      "[1544]\tvalid_0's multi_logloss: 1.405\n",
      "[1545]\tvalid_0's multi_logloss: 1.405\n",
      "[1546]\tvalid_0's multi_logloss: 1.40501\n",
      "[1547]\tvalid_0's multi_logloss: 1.40501\n",
      "[1548]\tvalid_0's multi_logloss: 1.40504\n",
      "[1549]\tvalid_0's multi_logloss: 1.40504\n",
      "[1550]\tvalid_0's multi_logloss: 1.40503\n",
      "[1551]\tvalid_0's multi_logloss: 1.40502\n",
      "[1552]\tvalid_0's multi_logloss: 1.40496\n",
      "[1553]\tvalid_0's multi_logloss: 1.40492\n",
      "[1554]\tvalid_0's multi_logloss: 1.40491\n",
      "[1555]\tvalid_0's multi_logloss: 1.40488\n",
      "[1556]\tvalid_0's multi_logloss: 1.40485\n",
      "[1557]\tvalid_0's multi_logloss: 1.40486\n",
      "[1558]\tvalid_0's multi_logloss: 1.40488\n",
      "[1559]\tvalid_0's multi_logloss: 1.40489\n",
      "[1560]\tvalid_0's multi_logloss: 1.40486\n",
      "[1561]\tvalid_0's multi_logloss: 1.40487\n",
      "[1562]\tvalid_0's multi_logloss: 1.40484\n",
      "[1563]\tvalid_0's multi_logloss: 1.40483\n",
      "[1564]\tvalid_0's multi_logloss: 1.40482\n",
      "[1565]\tvalid_0's multi_logloss: 1.4048\n",
      "[1566]\tvalid_0's multi_logloss: 1.40483\n",
      "[1567]\tvalid_0's multi_logloss: 1.40479\n",
      "[1568]\tvalid_0's multi_logloss: 1.40476\n",
      "[1569]\tvalid_0's multi_logloss: 1.40478\n",
      "[1570]\tvalid_0's multi_logloss: 1.40478\n",
      "[1571]\tvalid_0's multi_logloss: 1.4048\n",
      "[1572]\tvalid_0's multi_logloss: 1.4048\n",
      "[1573]\tvalid_0's multi_logloss: 1.4048\n",
      "[1574]\tvalid_0's multi_logloss: 1.40482\n",
      "[1575]\tvalid_0's multi_logloss: 1.4048\n",
      "[1576]\tvalid_0's multi_logloss: 1.40484\n",
      "[1577]\tvalid_0's multi_logloss: 1.40487\n",
      "[1578]\tvalid_0's multi_logloss: 1.40481\n",
      "[1579]\tvalid_0's multi_logloss: 1.4048\n",
      "[1580]\tvalid_0's multi_logloss: 1.40481\n",
      "[1581]\tvalid_0's multi_logloss: 1.40483\n",
      "[1582]\tvalid_0's multi_logloss: 1.40484\n",
      "[1583]\tvalid_0's multi_logloss: 1.40482\n",
      "[1584]\tvalid_0's multi_logloss: 1.40484\n",
      "[1585]\tvalid_0's multi_logloss: 1.40486\n",
      "[1586]\tvalid_0's multi_logloss: 1.40482\n",
      "[1587]\tvalid_0's multi_logloss: 1.40484\n",
      "[1588]\tvalid_0's multi_logloss: 1.40482\n",
      "[1589]\tvalid_0's multi_logloss: 1.40475\n",
      "[1590]\tvalid_0's multi_logloss: 1.40472\n",
      "[1591]\tvalid_0's multi_logloss: 1.40469\n",
      "[1592]\tvalid_0's multi_logloss: 1.40467\n",
      "[1593]\tvalid_0's multi_logloss: 1.40466\n",
      "[1594]\tvalid_0's multi_logloss: 1.40468\n",
      "[1595]\tvalid_0's multi_logloss: 1.40463\n",
      "[1596]\tvalid_0's multi_logloss: 1.40461\n",
      "[1597]\tvalid_0's multi_logloss: 1.40461\n",
      "[1598]\tvalid_0's multi_logloss: 1.40457\n",
      "[1599]\tvalid_0's multi_logloss: 1.40458\n",
      "[1600]\tvalid_0's multi_logloss: 1.40458\n",
      "[1601]\tvalid_0's multi_logloss: 1.40458\n",
      "[1602]\tvalid_0's multi_logloss: 1.40452\n",
      "[1603]\tvalid_0's multi_logloss: 1.40452\n",
      "[1604]\tvalid_0's multi_logloss: 1.40456\n",
      "[1605]\tvalid_0's multi_logloss: 1.40454\n",
      "[1606]\tvalid_0's multi_logloss: 1.40461\n",
      "[1607]\tvalid_0's multi_logloss: 1.40462\n",
      "[1608]\tvalid_0's multi_logloss: 1.40461\n",
      "[1609]\tvalid_0's multi_logloss: 1.40461\n",
      "[1610]\tvalid_0's multi_logloss: 1.40458\n",
      "[1611]\tvalid_0's multi_logloss: 1.4046\n",
      "[1612]\tvalid_0's multi_logloss: 1.40463\n",
      "[1613]\tvalid_0's multi_logloss: 1.40462\n",
      "[1614]\tvalid_0's multi_logloss: 1.40461\n",
      "[1615]\tvalid_0's multi_logloss: 1.40454\n",
      "[1616]\tvalid_0's multi_logloss: 1.40451\n",
      "[1617]\tvalid_0's multi_logloss: 1.40451\n",
      "[1618]\tvalid_0's multi_logloss: 1.40451\n",
      "[1619]\tvalid_0's multi_logloss: 1.40448\n",
      "[1620]\tvalid_0's multi_logloss: 1.40447\n",
      "[1621]\tvalid_0's multi_logloss: 1.4045\n",
      "[1622]\tvalid_0's multi_logloss: 1.40447\n",
      "[1623]\tvalid_0's multi_logloss: 1.40448\n",
      "[1624]\tvalid_0's multi_logloss: 1.40449\n",
      "[1625]\tvalid_0's multi_logloss: 1.40451\n",
      "[1626]\tvalid_0's multi_logloss: 1.4045\n",
      "[1627]\tvalid_0's multi_logloss: 1.40451\n",
      "[1628]\tvalid_0's multi_logloss: 1.40451\n",
      "[1629]\tvalid_0's multi_logloss: 1.40453\n",
      "[1630]\tvalid_0's multi_logloss: 1.40453\n",
      "[1631]\tvalid_0's multi_logloss: 1.40451\n",
      "[1632]\tvalid_0's multi_logloss: 1.40452\n",
      "[1633]\tvalid_0's multi_logloss: 1.40456\n",
      "[1634]\tvalid_0's multi_logloss: 1.40452\n",
      "[1635]\tvalid_0's multi_logloss: 1.40444\n",
      "[1636]\tvalid_0's multi_logloss: 1.40438\n",
      "[1637]\tvalid_0's multi_logloss: 1.40435\n",
      "[1638]\tvalid_0's multi_logloss: 1.40438\n",
      "[1639]\tvalid_0's multi_logloss: 1.40438\n",
      "[1640]\tvalid_0's multi_logloss: 1.40436\n",
      "[1641]\tvalid_0's multi_logloss: 1.40439\n",
      "[1642]\tvalid_0's multi_logloss: 1.40433\n",
      "[1643]\tvalid_0's multi_logloss: 1.40434\n",
      "[1644]\tvalid_0's multi_logloss: 1.40431\n",
      "[1645]\tvalid_0's multi_logloss: 1.40429\n",
      "[1646]\tvalid_0's multi_logloss: 1.40425\n",
      "[1647]\tvalid_0's multi_logloss: 1.40425\n",
      "[1648]\tvalid_0's multi_logloss: 1.4043\n",
      "[1649]\tvalid_0's multi_logloss: 1.40427\n",
      "[1650]\tvalid_0's multi_logloss: 1.40426\n",
      "[1651]\tvalid_0's multi_logloss: 1.40423\n",
      "[1652]\tvalid_0's multi_logloss: 1.40428\n",
      "[1653]\tvalid_0's multi_logloss: 1.40428\n",
      "[1654]\tvalid_0's multi_logloss: 1.40429\n",
      "[1655]\tvalid_0's multi_logloss: 1.40428\n",
      "[1656]\tvalid_0's multi_logloss: 1.40427\n",
      "[1657]\tvalid_0's multi_logloss: 1.40427\n",
      "[1658]\tvalid_0's multi_logloss: 1.40428\n",
      "[1659]\tvalid_0's multi_logloss: 1.40425\n",
      "[1660]\tvalid_0's multi_logloss: 1.40425\n",
      "[1661]\tvalid_0's multi_logloss: 1.40421\n",
      "[1662]\tvalid_0's multi_logloss: 1.40419\n",
      "[1663]\tvalid_0's multi_logloss: 1.40417\n",
      "[1664]\tvalid_0's multi_logloss: 1.40415\n",
      "[1665]\tvalid_0's multi_logloss: 1.40414\n",
      "[1666]\tvalid_0's multi_logloss: 1.40414\n",
      "[1667]\tvalid_0's multi_logloss: 1.40413\n",
      "[1668]\tvalid_0's multi_logloss: 1.40413\n",
      "[1669]\tvalid_0's multi_logloss: 1.40414\n",
      "[1670]\tvalid_0's multi_logloss: 1.40414\n",
      "[1671]\tvalid_0's multi_logloss: 1.40413\n",
      "[1672]\tvalid_0's multi_logloss: 1.40412\n",
      "[1673]\tvalid_0's multi_logloss: 1.40415\n",
      "[1674]\tvalid_0's multi_logloss: 1.40411\n",
      "[1675]\tvalid_0's multi_logloss: 1.40412\n",
      "[1676]\tvalid_0's multi_logloss: 1.40413\n",
      "[1677]\tvalid_0's multi_logloss: 1.40416\n",
      "[1678]\tvalid_0's multi_logloss: 1.40412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1679]\tvalid_0's multi_logloss: 1.40407\n",
      "[1680]\tvalid_0's multi_logloss: 1.40408\n",
      "[1681]\tvalid_0's multi_logloss: 1.40407\n",
      "[1682]\tvalid_0's multi_logloss: 1.4041\n",
      "[1683]\tvalid_0's multi_logloss: 1.40408\n",
      "[1684]\tvalid_0's multi_logloss: 1.40408\n",
      "[1685]\tvalid_0's multi_logloss: 1.40412\n",
      "[1686]\tvalid_0's multi_logloss: 1.40408\n",
      "[1687]\tvalid_0's multi_logloss: 1.40404\n",
      "[1688]\tvalid_0's multi_logloss: 1.40404\n",
      "[1689]\tvalid_0's multi_logloss: 1.40401\n",
      "[1690]\tvalid_0's multi_logloss: 1.40402\n",
      "[1691]\tvalid_0's multi_logloss: 1.40404\n",
      "[1692]\tvalid_0's multi_logloss: 1.40406\n",
      "[1693]\tvalid_0's multi_logloss: 1.40404\n",
      "[1694]\tvalid_0's multi_logloss: 1.40403\n",
      "[1695]\tvalid_0's multi_logloss: 1.40403\n",
      "[1696]\tvalid_0's multi_logloss: 1.40397\n",
      "[1697]\tvalid_0's multi_logloss: 1.40393\n",
      "[1698]\tvalid_0's multi_logloss: 1.40389\n",
      "[1699]\tvalid_0's multi_logloss: 1.40384\n",
      "[1700]\tvalid_0's multi_logloss: 1.40382\n",
      "[1701]\tvalid_0's multi_logloss: 1.40386\n",
      "[1702]\tvalid_0's multi_logloss: 1.40385\n",
      "[1703]\tvalid_0's multi_logloss: 1.40384\n",
      "[1704]\tvalid_0's multi_logloss: 1.40384\n",
      "[1705]\tvalid_0's multi_logloss: 1.40383\n",
      "[1706]\tvalid_0's multi_logloss: 1.40383\n",
      "[1707]\tvalid_0's multi_logloss: 1.40384\n",
      "[1708]\tvalid_0's multi_logloss: 1.40385\n",
      "[1709]\tvalid_0's multi_logloss: 1.4039\n",
      "[1710]\tvalid_0's multi_logloss: 1.40387\n",
      "[1711]\tvalid_0's multi_logloss: 1.40387\n",
      "[1712]\tvalid_0's multi_logloss: 1.40384\n",
      "[1713]\tvalid_0's multi_logloss: 1.40383\n",
      "[1714]\tvalid_0's multi_logloss: 1.40386\n",
      "[1715]\tvalid_0's multi_logloss: 1.40385\n",
      "[1716]\tvalid_0's multi_logloss: 1.40383\n",
      "[1717]\tvalid_0's multi_logloss: 1.40385\n",
      "[1718]\tvalid_0's multi_logloss: 1.40381\n",
      "[1719]\tvalid_0's multi_logloss: 1.40381\n",
      "[1720]\tvalid_0's multi_logloss: 1.40379\n",
      "[1721]\tvalid_0's multi_logloss: 1.40377\n",
      "[1722]\tvalid_0's multi_logloss: 1.40379\n",
      "[1723]\tvalid_0's multi_logloss: 1.40378\n",
      "[1724]\tvalid_0's multi_logloss: 1.40378\n",
      "[1725]\tvalid_0's multi_logloss: 1.40378\n",
      "[1726]\tvalid_0's multi_logloss: 1.40383\n",
      "[1727]\tvalid_0's multi_logloss: 1.40379\n",
      "[1728]\tvalid_0's multi_logloss: 1.40377\n",
      "[1729]\tvalid_0's multi_logloss: 1.40373\n",
      "[1730]\tvalid_0's multi_logloss: 1.40374\n",
      "[1731]\tvalid_0's multi_logloss: 1.4037\n",
      "[1732]\tvalid_0's multi_logloss: 1.4037\n",
      "[1733]\tvalid_0's multi_logloss: 1.40372\n",
      "[1734]\tvalid_0's multi_logloss: 1.40376\n",
      "[1735]\tvalid_0's multi_logloss: 1.40377\n",
      "[1736]\tvalid_0's multi_logloss: 1.40376\n",
      "[1737]\tvalid_0's multi_logloss: 1.40372\n",
      "[1738]\tvalid_0's multi_logloss: 1.40373\n",
      "[1739]\tvalid_0's multi_logloss: 1.40372\n",
      "[1740]\tvalid_0's multi_logloss: 1.40376\n",
      "[1741]\tvalid_0's multi_logloss: 1.40377\n",
      "[1742]\tvalid_0's multi_logloss: 1.40374\n",
      "[1743]\tvalid_0's multi_logloss: 1.40373\n",
      "[1744]\tvalid_0's multi_logloss: 1.40373\n",
      "[1745]\tvalid_0's multi_logloss: 1.40372\n",
      "[1746]\tvalid_0's multi_logloss: 1.40368\n",
      "[1747]\tvalid_0's multi_logloss: 1.40369\n",
      "[1748]\tvalid_0's multi_logloss: 1.40368\n",
      "[1749]\tvalid_0's multi_logloss: 1.40367\n",
      "[1750]\tvalid_0's multi_logloss: 1.4037\n",
      "[1751]\tvalid_0's multi_logloss: 1.40369\n",
      "[1752]\tvalid_0's multi_logloss: 1.4037\n",
      "[1753]\tvalid_0's multi_logloss: 1.40371\n",
      "[1754]\tvalid_0's multi_logloss: 1.40369\n",
      "[1755]\tvalid_0's multi_logloss: 1.40372\n",
      "[1756]\tvalid_0's multi_logloss: 1.40367\n",
      "[1757]\tvalid_0's multi_logloss: 1.40365\n",
      "[1758]\tvalid_0's multi_logloss: 1.4036\n",
      "[1759]\tvalid_0's multi_logloss: 1.40362\n",
      "[1760]\tvalid_0's multi_logloss: 1.40362\n",
      "[1761]\tvalid_0's multi_logloss: 1.40362\n",
      "[1762]\tvalid_0's multi_logloss: 1.40357\n",
      "[1763]\tvalid_0's multi_logloss: 1.40354\n",
      "[1764]\tvalid_0's multi_logloss: 1.40352\n",
      "[1765]\tvalid_0's multi_logloss: 1.40347\n",
      "[1766]\tvalid_0's multi_logloss: 1.40344\n",
      "[1767]\tvalid_0's multi_logloss: 1.40336\n",
      "[1768]\tvalid_0's multi_logloss: 1.40336\n",
      "[1769]\tvalid_0's multi_logloss: 1.40332\n",
      "[1770]\tvalid_0's multi_logloss: 1.4033\n",
      "[1771]\tvalid_0's multi_logloss: 1.4033\n",
      "[1772]\tvalid_0's multi_logloss: 1.40329\n",
      "[1773]\tvalid_0's multi_logloss: 1.40325\n",
      "[1774]\tvalid_0's multi_logloss: 1.4033\n",
      "[1775]\tvalid_0's multi_logloss: 1.4033\n",
      "[1776]\tvalid_0's multi_logloss: 1.40327\n",
      "[1777]\tvalid_0's multi_logloss: 1.40327\n",
      "[1778]\tvalid_0's multi_logloss: 1.40327\n",
      "[1779]\tvalid_0's multi_logloss: 1.40328\n",
      "[1780]\tvalid_0's multi_logloss: 1.4033\n",
      "[1781]\tvalid_0's multi_logloss: 1.40329\n",
      "[1782]\tvalid_0's multi_logloss: 1.40329\n",
      "[1783]\tvalid_0's multi_logloss: 1.40331\n",
      "[1784]\tvalid_0's multi_logloss: 1.4033\n",
      "[1785]\tvalid_0's multi_logloss: 1.40327\n",
      "[1786]\tvalid_0's multi_logloss: 1.40328\n",
      "[1787]\tvalid_0's multi_logloss: 1.40331\n",
      "[1788]\tvalid_0's multi_logloss: 1.4033\n",
      "[1789]\tvalid_0's multi_logloss: 1.4033\n",
      "[1790]\tvalid_0's multi_logloss: 1.40332\n",
      "[1791]\tvalid_0's multi_logloss: 1.40333\n",
      "[1792]\tvalid_0's multi_logloss: 1.40333\n",
      "[1793]\tvalid_0's multi_logloss: 1.40334\n",
      "[1794]\tvalid_0's multi_logloss: 1.40332\n",
      "[1795]\tvalid_0's multi_logloss: 1.40333\n",
      "[1796]\tvalid_0's multi_logloss: 1.40331\n",
      "[1797]\tvalid_0's multi_logloss: 1.40334\n",
      "[1798]\tvalid_0's multi_logloss: 1.40334\n",
      "[1799]\tvalid_0's multi_logloss: 1.40331\n",
      "[1800]\tvalid_0's multi_logloss: 1.40329\n",
      "[1801]\tvalid_0's multi_logloss: 1.40326\n",
      "[1802]\tvalid_0's multi_logloss: 1.40329\n",
      "[1803]\tvalid_0's multi_logloss: 1.40324\n",
      "[1804]\tvalid_0's multi_logloss: 1.40328\n",
      "[1805]\tvalid_0's multi_logloss: 1.40324\n",
      "[1806]\tvalid_0's multi_logloss: 1.40323\n",
      "[1807]\tvalid_0's multi_logloss: 1.40325\n",
      "[1808]\tvalid_0's multi_logloss: 1.40326\n",
      "[1809]\tvalid_0's multi_logloss: 1.40326\n",
      "[1810]\tvalid_0's multi_logloss: 1.40323\n",
      "[1811]\tvalid_0's multi_logloss: 1.40322\n",
      "[1812]\tvalid_0's multi_logloss: 1.40322\n",
      "[1813]\tvalid_0's multi_logloss: 1.40322\n",
      "[1814]\tvalid_0's multi_logloss: 1.40327\n",
      "[1815]\tvalid_0's multi_logloss: 1.40321\n",
      "[1816]\tvalid_0's multi_logloss: 1.40325\n",
      "[1817]\tvalid_0's multi_logloss: 1.40327\n",
      "[1818]\tvalid_0's multi_logloss: 1.40327\n",
      "[1819]\tvalid_0's multi_logloss: 1.40335\n",
      "[1820]\tvalid_0's multi_logloss: 1.40329\n",
      "[1821]\tvalid_0's multi_logloss: 1.40324\n",
      "[1822]\tvalid_0's multi_logloss: 1.40321\n",
      "[1823]\tvalid_0's multi_logloss: 1.40319\n",
      "[1824]\tvalid_0's multi_logloss: 1.40319\n",
      "[1825]\tvalid_0's multi_logloss: 1.40318\n",
      "[1826]\tvalid_0's multi_logloss: 1.40317\n",
      "[1827]\tvalid_0's multi_logloss: 1.40314\n",
      "[1828]\tvalid_0's multi_logloss: 1.40316\n",
      "[1829]\tvalid_0's multi_logloss: 1.40316\n",
      "[1830]\tvalid_0's multi_logloss: 1.40317\n",
      "[1831]\tvalid_0's multi_logloss: 1.40317\n",
      "[1832]\tvalid_0's multi_logloss: 1.40318\n",
      "[1833]\tvalid_0's multi_logloss: 1.40319\n",
      "[1834]\tvalid_0's multi_logloss: 1.4032\n",
      "[1835]\tvalid_0's multi_logloss: 1.4032\n",
      "[1836]\tvalid_0's multi_logloss: 1.4032\n",
      "[1837]\tvalid_0's multi_logloss: 1.40319\n",
      "[1838]\tvalid_0's multi_logloss: 1.40325\n",
      "[1839]\tvalid_0's multi_logloss: 1.40322\n",
      "[1840]\tvalid_0's multi_logloss: 1.40326\n",
      "[1841]\tvalid_0's multi_logloss: 1.40325\n",
      "[1842]\tvalid_0's multi_logloss: 1.40323\n",
      "[1843]\tvalid_0's multi_logloss: 1.40325\n",
      "[1844]\tvalid_0's multi_logloss: 1.40322\n",
      "[1845]\tvalid_0's multi_logloss: 1.40319\n",
      "[1846]\tvalid_0's multi_logloss: 1.4032\n",
      "[1847]\tvalid_0's multi_logloss: 1.40318\n",
      "[1848]\tvalid_0's multi_logloss: 1.40314\n",
      "[1849]\tvalid_0's multi_logloss: 1.40312\n",
      "[1850]\tvalid_0's multi_logloss: 1.40314\n",
      "[1851]\tvalid_0's multi_logloss: 1.40311\n",
      "[1852]\tvalid_0's multi_logloss: 1.4031\n",
      "[1853]\tvalid_0's multi_logloss: 1.40306\n",
      "[1854]\tvalid_0's multi_logloss: 1.40303\n",
      "[1855]\tvalid_0's multi_logloss: 1.40304\n",
      "[1856]\tvalid_0's multi_logloss: 1.40301\n",
      "[1857]\tvalid_0's multi_logloss: 1.40303\n",
      "[1858]\tvalid_0's multi_logloss: 1.40304\n",
      "[1859]\tvalid_0's multi_logloss: 1.40306\n",
      "[1860]\tvalid_0's multi_logloss: 1.40303\n",
      "[1861]\tvalid_0's multi_logloss: 1.40302\n",
      "[1862]\tvalid_0's multi_logloss: 1.40301\n",
      "[1863]\tvalid_0's multi_logloss: 1.40303\n",
      "[1864]\tvalid_0's multi_logloss: 1.40302\n",
      "[1865]\tvalid_0's multi_logloss: 1.40303\n",
      "[1866]\tvalid_0's multi_logloss: 1.40304\n",
      "[1867]\tvalid_0's multi_logloss: 1.40304\n",
      "[1868]\tvalid_0's multi_logloss: 1.40302\n",
      "[1869]\tvalid_0's multi_logloss: 1.403\n",
      "[1870]\tvalid_0's multi_logloss: 1.40298\n",
      "[1871]\tvalid_0's multi_logloss: 1.40297\n",
      "[1872]\tvalid_0's multi_logloss: 1.40297\n",
      "[1873]\tvalid_0's multi_logloss: 1.4029\n",
      "[1874]\tvalid_0's multi_logloss: 1.40292\n",
      "[1875]\tvalid_0's multi_logloss: 1.40293\n",
      "[1876]\tvalid_0's multi_logloss: 1.40288\n",
      "[1877]\tvalid_0's multi_logloss: 1.40288\n",
      "[1878]\tvalid_0's multi_logloss: 1.40287\n",
      "[1879]\tvalid_0's multi_logloss: 1.40286\n",
      "[1880]\tvalid_0's multi_logloss: 1.40284\n",
      "[1881]\tvalid_0's multi_logloss: 1.40282\n",
      "[1882]\tvalid_0's multi_logloss: 1.40291\n",
      "[1883]\tvalid_0's multi_logloss: 1.40288\n",
      "[1884]\tvalid_0's multi_logloss: 1.40285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1885]\tvalid_0's multi_logloss: 1.40286\n",
      "[1886]\tvalid_0's multi_logloss: 1.40289\n",
      "[1887]\tvalid_0's multi_logloss: 1.40288\n",
      "[1888]\tvalid_0's multi_logloss: 1.4028\n",
      "[1889]\tvalid_0's multi_logloss: 1.4028\n",
      "[1890]\tvalid_0's multi_logloss: 1.40282\n",
      "[1891]\tvalid_0's multi_logloss: 1.4028\n",
      "[1892]\tvalid_0's multi_logloss: 1.4028\n",
      "[1893]\tvalid_0's multi_logloss: 1.40282\n",
      "[1894]\tvalid_0's multi_logloss: 1.40282\n",
      "[1895]\tvalid_0's multi_logloss: 1.40283\n",
      "[1896]\tvalid_0's multi_logloss: 1.40279\n",
      "[1897]\tvalid_0's multi_logloss: 1.40281\n",
      "[1898]\tvalid_0's multi_logloss: 1.40284\n",
      "[1899]\tvalid_0's multi_logloss: 1.40281\n",
      "[1900]\tvalid_0's multi_logloss: 1.40277\n",
      "[1901]\tvalid_0's multi_logloss: 1.40275\n",
      "[1902]\tvalid_0's multi_logloss: 1.40275\n",
      "[1903]\tvalid_0's multi_logloss: 1.40273\n",
      "[1904]\tvalid_0's multi_logloss: 1.40272\n",
      "[1905]\tvalid_0's multi_logloss: 1.40271\n",
      "[1906]\tvalid_0's multi_logloss: 1.40271\n",
      "[1907]\tvalid_0's multi_logloss: 1.40271\n",
      "[1908]\tvalid_0's multi_logloss: 1.40269\n",
      "[1909]\tvalid_0's multi_logloss: 1.40266\n",
      "[1910]\tvalid_0's multi_logloss: 1.40269\n",
      "[1911]\tvalid_0's multi_logloss: 1.40269\n",
      "[1912]\tvalid_0's multi_logloss: 1.4027\n",
      "[1913]\tvalid_0's multi_logloss: 1.40271\n",
      "[1914]\tvalid_0's multi_logloss: 1.40274\n",
      "[1915]\tvalid_0's multi_logloss: 1.40273\n",
      "[1916]\tvalid_0's multi_logloss: 1.40269\n",
      "[1917]\tvalid_0's multi_logloss: 1.40274\n",
      "[1918]\tvalid_0's multi_logloss: 1.40273\n",
      "[1919]\tvalid_0's multi_logloss: 1.40273\n",
      "[1920]\tvalid_0's multi_logloss: 1.40275\n",
      "[1921]\tvalid_0's multi_logloss: 1.40279\n",
      "[1922]\tvalid_0's multi_logloss: 1.40281\n",
      "[1923]\tvalid_0's multi_logloss: 1.40277\n",
      "[1924]\tvalid_0's multi_logloss: 1.40276\n",
      "[1925]\tvalid_0's multi_logloss: 1.40279\n",
      "[1926]\tvalid_0's multi_logloss: 1.40278\n",
      "[1927]\tvalid_0's multi_logloss: 1.40278\n",
      "[1928]\tvalid_0's multi_logloss: 1.40275\n",
      "[1929]\tvalid_0's multi_logloss: 1.40274\n",
      "[1930]\tvalid_0's multi_logloss: 1.40269\n",
      "[1931]\tvalid_0's multi_logloss: 1.40267\n",
      "[1932]\tvalid_0's multi_logloss: 1.40262\n",
      "[1933]\tvalid_0's multi_logloss: 1.40259\n",
      "[1934]\tvalid_0's multi_logloss: 1.4026\n",
      "[1935]\tvalid_0's multi_logloss: 1.40266\n",
      "[1936]\tvalid_0's multi_logloss: 1.40266\n",
      "[1937]\tvalid_0's multi_logloss: 1.40262\n",
      "[1938]\tvalid_0's multi_logloss: 1.4026\n",
      "[1939]\tvalid_0's multi_logloss: 1.40259\n",
      "[1940]\tvalid_0's multi_logloss: 1.40257\n",
      "[1941]\tvalid_0's multi_logloss: 1.40255\n",
      "[1942]\tvalid_0's multi_logloss: 1.40253\n",
      "[1943]\tvalid_0's multi_logloss: 1.4025\n",
      "[1944]\tvalid_0's multi_logloss: 1.40252\n",
      "[1945]\tvalid_0's multi_logloss: 1.40252\n",
      "[1946]\tvalid_0's multi_logloss: 1.40249\n",
      "[1947]\tvalid_0's multi_logloss: 1.4025\n",
      "[1948]\tvalid_0's multi_logloss: 1.40249\n",
      "[1949]\tvalid_0's multi_logloss: 1.40248\n",
      "[1950]\tvalid_0's multi_logloss: 1.40248\n",
      "[1951]\tvalid_0's multi_logloss: 1.40245\n",
      "[1952]\tvalid_0's multi_logloss: 1.40244\n",
      "[1953]\tvalid_0's multi_logloss: 1.40242\n",
      "[1954]\tvalid_0's multi_logloss: 1.40244\n",
      "[1955]\tvalid_0's multi_logloss: 1.40245\n",
      "[1956]\tvalid_0's multi_logloss: 1.40249\n",
      "[1957]\tvalid_0's multi_logloss: 1.40248\n",
      "[1958]\tvalid_0's multi_logloss: 1.40246\n",
      "[1959]\tvalid_0's multi_logloss: 1.40247\n",
      "[1960]\tvalid_0's multi_logloss: 1.40247\n",
      "[1961]\tvalid_0's multi_logloss: 1.40249\n",
      "[1962]\tvalid_0's multi_logloss: 1.40252\n",
      "[1963]\tvalid_0's multi_logloss: 1.40251\n",
      "[1964]\tvalid_0's multi_logloss: 1.40251\n",
      "[1965]\tvalid_0's multi_logloss: 1.40252\n",
      "[1966]\tvalid_0's multi_logloss: 1.40249\n",
      "[1967]\tvalid_0's multi_logloss: 1.40246\n",
      "[1968]\tvalid_0's multi_logloss: 1.40246\n",
      "[1969]\tvalid_0's multi_logloss: 1.40246\n",
      "[1970]\tvalid_0's multi_logloss: 1.40247\n",
      "[1971]\tvalid_0's multi_logloss: 1.40241\n",
      "[1972]\tvalid_0's multi_logloss: 1.40237\n",
      "[1973]\tvalid_0's multi_logloss: 1.40233\n",
      "[1974]\tvalid_0's multi_logloss: 1.40231\n",
      "[1975]\tvalid_0's multi_logloss: 1.40233\n",
      "[1976]\tvalid_0's multi_logloss: 1.40229\n",
      "[1977]\tvalid_0's multi_logloss: 1.40231\n",
      "[1978]\tvalid_0's multi_logloss: 1.40228\n",
      "[1979]\tvalid_0's multi_logloss: 1.40226\n",
      "[1980]\tvalid_0's multi_logloss: 1.40228\n",
      "[1981]\tvalid_0's multi_logloss: 1.40228\n",
      "[1982]\tvalid_0's multi_logloss: 1.40229\n",
      "[1983]\tvalid_0's multi_logloss: 1.40232\n",
      "[1984]\tvalid_0's multi_logloss: 1.40232\n",
      "[1985]\tvalid_0's multi_logloss: 1.40233\n",
      "[1986]\tvalid_0's multi_logloss: 1.40228\n",
      "[1987]\tvalid_0's multi_logloss: 1.40228\n",
      "[1988]\tvalid_0's multi_logloss: 1.40231\n",
      "[1989]\tvalid_0's multi_logloss: 1.40235\n",
      "[1990]\tvalid_0's multi_logloss: 1.40233\n",
      "[1991]\tvalid_0's multi_logloss: 1.40231\n",
      "[1992]\tvalid_0's multi_logloss: 1.40233\n",
      "[1993]\tvalid_0's multi_logloss: 1.40232\n",
      "[1994]\tvalid_0's multi_logloss: 1.40235\n",
      "[1995]\tvalid_0's multi_logloss: 1.40233\n",
      "[1996]\tvalid_0's multi_logloss: 1.40234\n",
      "[1997]\tvalid_0's multi_logloss: 1.40235\n",
      "[1998]\tvalid_0's multi_logloss: 1.40238\n",
      "[1999]\tvalid_0's multi_logloss: 1.40237\n",
      "[2000]\tvalid_0's multi_logloss: 1.40238\n",
      "accuracy et f1-score pour model optimal train 0.4579822097378277 0.430002343801422\n",
      "accuracy et f1-score pour model optimal val 0.39146230699364215 0.3646052288767023\n"
     ]
    }
   ],
   "source": [
    "# BONUS!\n",
    "# 5 - Try to improve performance with another classifier\n",
    "#     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "lgbm = lgb.LGBMClassifier(objective='multiclass', n_estimators=2000, reg_lambda=5000)\n",
    "print(train_data.shape)\n",
    "lgbm.fit(train_embeddings_norm, train_labels, eval_set=[(val_embeddings_norm, val_labels)])\n",
    "acc_train, f1_train = metrics(clf, train_data, train_labels)\n",
    "acc, f1 = metrics(clf, val_data, val_labels)\n",
    "print('accuracy et f1-score pour model optimal train', acc_train, f1_train)\n",
    "print('accuracy et f1-score pour model optimal val', acc, f1)\n",
    "        \n",
    "\n",
    "prediction_test = lgbm.predict(test_embeddings_norm)\n",
    "with open('./lgb_bov_y_test_sst.txt', 'w') as f1:\n",
    "    for k in prediction_test:\n",
    "        f1.write(str(k) + os.linesep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sentence classification with LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test sets of SST\n",
    "\n",
    "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
    "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "\n",
    "n_classes=5\n",
    "\n",
    "words=[]\n",
    "sentences_train = []\n",
    "Y_train = []\n",
    "file = open(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.train'), \"r\")\n",
    "for row in file:\n",
    "    sentences_train.append(row[1:])\n",
    "    Y_train.append(row[0])\n",
    "    for w in row.split()[1:]:\n",
    "        words.append(w)\n",
    "\n",
    "sentences_valid = []\n",
    "Y_valid = []\n",
    "file = open(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.dev'), \"r\")\n",
    "for row in file:\n",
    "    sentences_valid.append(row[1:])\n",
    "    Y_valid.append(row[0])\n",
    "    for w in row.split()[1:]:\n",
    "        words.append(w)\n",
    "\n",
    "sentences_test = []\n",
    "file = open(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.test.X'), \"r\")\n",
    "for row in file:\n",
    "    sentences_test.append(row)\n",
    "    for w in row.split():\n",
    "        words.append(w)\n",
    "        \n",
    "        \n",
    "Y_valid_temp = [int(l) for l in Y_valid]\n",
    "Y_train_temp = [int(l) for l in Y_train]\n",
    "\n",
    "\n",
    "Y_train = to_categorical(Y_train_temp, num_classes=n_classes)\n",
    "Y_val = to_categorical(Y_valid_temp, num_classes=n_classes)\n",
    "\n",
    "vocab_size = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Transform text to integers using keras.preprocessing.text.one_hot function\n",
    "#     https://keras.io/preprocessing/text/\n",
    "\n",
    "# TYPE CODE HERE\n",
    "X_train = [one_hot(sentence, vocab_size) for sentence in sentences_train]\n",
    "X_valid = [one_hot(sentence, vocab_size) for sentence in sentences_valid]\n",
    "X_test = [one_hot(sentence, vocab_size) for sentence in sentences_test]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding input data**\n",
    "\n",
    "Models in Keras (and elsewhere) take batches of sentences of the same length as input. It is because Deep Learning framework have been designed to handle well Tensors, which are particularly suited for fast computation on the GPU.\n",
    "\n",
    "Since sentences have different sizes, we \"pad\" them. That is, we add dummy \"padding\" tokens so that they all have the same length.\n",
    "\n",
    "The input to a Keras model thus has this size : (batchsize, maxseqlen) where maxseqlen is the maximum length of a sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Pad your sequences using keras.preprocessing.sequence.pad_sequences\n",
    "#     https://keras.io/preprocessing/sequence/\n",
    "\n",
    "# TYPE CODE HERE\n",
    "max_len = 50\n",
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train, max_len)\n",
    "X_val = keras.preprocessing.sequence.pad_sequences(X_valid, max_len)\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Design and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, dropout=0.4, recurrent_dropout=0.4)`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# 4 - Design your encoder + classifier using keras.layers\n",
    "#     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
    "#     Then we add components to this contained : the lookuptable, the LSTM, the classifier etc.\n",
    "#     All of these components are contained in the Sequential() and are trained together.\n",
    "\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "embed_dim  = 32  # word embedding dimension\n",
    "nhid       = 64  # number of hidden units in the LSTM\n",
    "vocab_size = len(words)  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim))\n",
    "model.add(LSTM(nhid, dropout_W=0.4, dropout_U=0.4))\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 32)          7271840   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 7,296,997\n",
      "Trainable params: 7,296,997\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:928: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` insted.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "# 5 - Define your loss/optimizer/metrics\n",
    "\n",
    "# MODIFY CODE BELOW\n",
    "loss_classif     =  'categorical_crossentropy' # find the right loss for multi-class classification\n",
    "optimizer        =  'adam' # find the right optimizer\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint('.mdl_lstm.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/6\n",
      "8544/8544 [==============================] - 29s 3ms/step - loss: 1.5767 - acc: 0.2729 - val_loss: 1.5695 - val_acc: 0.2534\n",
      "Epoch 2/6\n",
      "8544/8544 [==============================] - 35s 4ms/step - loss: 1.5292 - acc: 0.3173 - val_loss: 1.4684 - val_acc: 0.3542\n",
      "Epoch 3/6\n",
      "8544/8544 [==============================] - 31s 4ms/step - loss: 1.3234 - acc: 0.4129 - val_loss: 1.3829 - val_acc: 0.3787\n",
      "Epoch 4/6\n",
      "8544/8544 [==============================] - 61s 7ms/step - loss: 1.1351 - acc: 0.4712 - val_loss: 1.4328 - val_acc: 0.3787\n",
      "Epoch 5/6\n",
      "8544/8544 [==============================] - 42s 5ms/step - loss: 0.9838 - acc: 0.5301 - val_loss: 1.5146 - val_acc: 0.3724\n",
      "Epoch 6/6\n",
      "8544/8544 [==============================] - 29s 3ms/step - loss: 0.8719 - acc: 0.5765 - val_loss: 1.6364 - val_acc: 0.3606\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6 - Train your model and find the best hyperparameters for your dev set\n",
    "#     you will be evaluated on the quality of your predictions on the test set\n",
    "\n",
    "\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "bs = 64\n",
    "n_epochs = 6\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size=bs, nb_epoch=n_epochs, validation_data=(X_val, Y_val), callbacks=[earlyStopping, mcp_save, reduce_lr_loss])\n",
    "\n",
    "#Model Visualisation\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,1])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 - Generate your predictions on the test set using model.predict(x_test)\n",
    "#     https://keras.io/models/model/\n",
    "#     Log your predictions in a file (one line = one integer: 0,1,2,3,4)\n",
    "#     Attach the output file \"logreg_lstm_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "\n",
    "\n",
    "# TYPE CODE HERE\n",
    "model.load_weights(filepath = '.mdl_lstm.hdf5')\n",
    "pred_test = np.argmax(model.predict(X_test),axis=1)\n",
    "with open('./logreg_lstm_y_test_sst.txt', 'w') as f1:\n",
    "    for k in pred_test:\n",
    "        f1.write(str(k) + os.linesep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 -- innovate !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:928: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` insted.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:61: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/6\n",
      "8544/8544 [==============================] - 62s 7ms/step - loss: 1.5553 - acc: 0.2915 - val_loss: 1.5100 - val_acc: 0.3333\n",
      "Epoch 2/6\n",
      "8544/8544 [==============================] - 54s 6ms/step - loss: 1.3453 - acc: 0.4221 - val_loss: 1.3439 - val_acc: 0.3978\n",
      "Epoch 3/6\n",
      "8544/8544 [==============================] - 39s 5ms/step - loss: 1.0030 - acc: 0.5902 - val_loss: 1.4191 - val_acc: 0.3769\n",
      "Epoch 4/6\n",
      "8544/8544 [==============================] - 37s 4ms/step - loss: 0.6501 - acc: 0.7734 - val_loss: 1.6679 - val_acc: 0.3815\n",
      "Epoch 5/6\n",
      "8544/8544 [==============================] - 41s 5ms/step - loss: 0.3420 - acc: 0.8997 - val_loss: 2.0387 - val_acc: 0.3833\n",
      "Epoch 6/6\n",
      "8544/8544 [==============================] - 38s 4ms/step - loss: 0.1759 - acc: 0.9524 - val_loss: 2.3708 - val_acc: 0.3597\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 8 - Open question: find a model that is better on your dev set\n",
    "#     (e.g: use a 1D ConvNet, use a better classifier, pretrain your lookup tables ..)\n",
    "#     you will get point if the results on the test set are better: be careful of not overfitting your dev set too much..\n",
    "#     Attach the output file \"XXX_XXX_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE\n",
    "\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D, Conv1D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "# Model Hyperparameters\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "bs = 64\n",
    "n_epochs = 6\n",
    "\n",
    "embedding_dim  = 32  # word embedding dimension\n",
    "vocabulary_size = len(words)  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "sequence_length = 50\n",
    "num_filters = 512\n",
    "filter_sizes = [3,4,5]\n",
    "drop = 0.5\n",
    "\n",
    "# this returns a tensor\n",
    "print(\"Creating Model...\")\n",
    "inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "embedding = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n",
    "reshape = Reshape((sequence_length,embedding_dim,1))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "model_output = Dense(5, activation=\"softmax\")(dropout)\n",
    "model = Model(inputs, model_output)\n",
    "\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint('.mdl_proposed.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size=bs, nb_epoch=n_epochs, validation_data=(X_val, Y_val), callbacks=[earlyStopping, mcp_save, reduce_lr_loss])\n",
    "\n",
    "#Model Visualisation\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,1])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# TYPE CODE HERE\n",
    "model.load_weights(filepath = '.mdl_proposed.hdf5')\n",
    "pred_test = np.argmax(model.predict(X_test),axis=1)\n",
    "with open('./proposed_y_test_sst.txt', 'w') as f1:\n",
    "    for k in pred_test:\n",
    "        f1.write(str(k) + os.linesep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
